

---

# ðŸ“˜ MNIST Digit Classification â€“ Full Minimal Working Code (Keras)

---

## ðŸ”¹ Step 1: Import Required Libraries (Minimal)

```python
import tensorflow as tf
from tensorflow.keras.datasets import mnist
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten
```

---

## ðŸ”¹ Step 2: Load MNIST Dataset

```python
(x_train, y_train), (x_test, y_test) = mnist.load_data()
```

ðŸ“Œ Dataset info:

* Image size: `28 Ã— 28`
* Classes: `0â€“9` (10 classes)

---

## ðŸ”¹ Step 3: Preprocess Data (Mandatory)

### Normalize images

```python
x_train = x_train / 255.0
x_test  = x_test / 255.0
```

ðŸ“Œ Reason:

* Pixel range `0â€“255` â†’ `0â€“1`
* Faster & stable training

---

## ðŸ”¹ Step 4: Build Model (Sequential, Minimal)

```python
model = Sequential([
    Flatten(input_shape=(28, 28)),
    Dense(32, activation='relu'),
    Dense(10, activation='softmax')
])
```

ðŸ“Œ Explanation:

* `Flatten` â†’ image â†’ 1D vector (784)
* `Dense(32)` â†’ hidden layer
* `Dense(10)` â†’ 10 digit classes

---

## ðŸ”¹ Step 5: Compile Model (MANDATORY)

```python
model.compile(
    optimizer='adam',
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)
```

ðŸ“Œ Minimum required:

* optimizer
* loss
* metrics (optional but practical)

---

## ðŸ”¹ Step 6: Train Model (fit)

```python
model.fit(
    x_train,
    y_train,
    epochs=5
)
```

ðŸ“Œ Minimum required:

* training data
* labels
* epochs

---

## ðŸ”¹ Step 7: Evaluate Model

```python
test_loss, test_accuracy = model.evaluate(x_test, y_test)
print("Test Accuracy:", test_accuracy)
```

ðŸ“Œ Purpose:

* Model à¦•à¦¤à¦Ÿà¦¾ à¦­à¦¾à¦²à§‹ à¦•à¦¾à¦œ à¦•à¦°à¦›à§‡ à¦¤à¦¾ à¦¦à§‡à¦–à¦¾

---

## ðŸ”¹ Step 8: Predict (Single Sample)

```python
predictions = model.predict(x_test)
```

### Predict one image

```python
import numpy as np

predicted_label = np.argmax(predictions[0])
true_label = y_test[0]

print("Predicted:", predicted_label)
print("Actual:", true_label)
```

ðŸ“Œ `argmax` â†’ highest probability class

---

# ðŸ§  Minimal Parameters Summary (Exam Gold â­)

| Function       | Mandatory Parameters |
| -------------- | -------------------- |
| `Sequential()` | layers               |
| `Dense()`      | units                |
| `compile()`    | optimizer, loss      |
| `fit()`        | x, y, epochs         |
| `evaluate()`   | x, y                 |
| `predict()`    | x                    |

---

# ðŸ§ª One-Block Complete Code (Copyâ€“Paste Ready)

```python
import tensorflow as tf
import numpy as np
from tensorflow.keras.datasets import mnist
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten

# Load data
(x_train, y_train), (x_test, y_test) = mnist.load_data()

# Normalize
x_train = x_train / 255.0
x_test  = x_test / 255.0

# Build model
model = Sequential([
    Flatten(input_shape=(28, 28)),
    Dense(32, activation='relu'),
    Dense(10, activation='softmax')
])

# Compile
model.compile(
    optimizer='adam',
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

# Train
model.fit(x_train, y_train, epochs=5)

# Evaluate
loss, acc = model.evaluate(x_test, y_test)
print("Test Accuracy:", acc)

# Predict
pred = model.predict(x_test)
print("Predicted label:", np.argmax(pred[0]))
print("Actual label:", y_test[0])
```

---

## âœ… Final Notes (Very Important)

* âœ” MNIST labels integer â†’ `sparse_categorical_crossentropy`
* âœ” Softmax output units = number of classes
* âœ” Sequential best for this problem
* âœ” This is **minimum working deep learning pipeline**

---



---

# ðŸ“˜ MNIST Digit Classification

## Minimal Parameters + Accuracy/Loss Curve + 10 Image Prediction

---

## ðŸ”¹ Full Working Code (Minimal but Complete)

```python
import numpy as np
import matplotlib.pyplot as plt

from tensorflow.keras.datasets import mnist
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten
```

---

## ðŸ”¹ 1. Load Dataset (No parameter needed)

```python
(x_train, y_train), (x_test, y_test) = mnist.load_data()
```

---

## ðŸ”¹ 2. Normalize Data (Mandatory preprocessing)

```python
x_train = x_train / 255.0
x_test  = x_test / 255.0
```

---

## ðŸ”¹ 3. Build Model (Minimal)

```python
model = Sequential([
    Flatten(input_shape=(28, 28)),
    Dense(32, activation='relu'),
    Dense(10, activation='softmax')
])
```

---

## ðŸ”¹ 4. Compile Model (Minimal required)

```python
model.compile(
    optimizer='adam',
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)
```

---

## ðŸ”¹ 5. Train Model (fit â†’ returns History object)

```python
history = model.fit(
    x_train,
    y_train,
    epochs=5
)
```

ðŸ“Œ à¦à¦–à¦¾à¦¨à§‡ **history** object à¦ªà¦¾à¦“à§Ÿà¦¾ à¦—à§‡à¦›à§‡
à¦à¦Ÿà¦¾à¦‡ à¦¦à¦¿à§Ÿà§‡ curve à¦†à¦à¦•à¦¬à§‹

---

## ðŸ”¹ 6. Plot Accuracy & Loss Curve (from History)

```python
plt.figure(figsize=(12,4))

# Loss curve
plt.subplot(1,2,1)
plt.plot(history.history['loss'])
plt.title('Training Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')

# Accuracy curve
plt.subplot(1,2,2)
plt.plot(history.history['accuracy'])
plt.title('Training Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')

plt.show()
```

ðŸ“Œ à¦•à§‹à¦¨à§‹ extra parameter à¦›à¦¾à§œà¦¾à¦‡ curve à¦ªà¦¾à¦“à§Ÿà¦¾ à¦¯à¦¾à¦šà§à¦›à§‡

---

## ðŸ”¹ 7. Evaluate Model (Minimal)

```python
loss, accuracy = model.evaluate(x_test, y_test)

print("Test Loss:", loss)
print("Test Accuracy:", accuracy)
```

---

## ðŸ”¹ 8. Predict on Test Data (Minimal)

```python
predictions = model.predict(x_test)
```

ðŸ“Œ `predictions.shape = (10000, 10)`

---

## ðŸ”¹ 9. Display 10 Images with Prediction & Probability

```python
plt.figure(figsize=(12,4))

for i in range(10):
    plt.subplot(2,5,i+1)
    plt.imshow(x_test[i], cmap='gray')
    
    predicted_class = np.argmax(predictions[i])
    probability = np.max(predictions[i])
    
    plt.title(f"Pred: {predicted_class}\nProb: {probability:.2f}")
    plt.axis('off')

plt.show()
```

---

# ðŸ§  à¦•à§€ à¦•à§€ Minimal Parameter à¦¬à§à¦¯à¦¬à¦¹à¦¾à¦° à¦¹à§Ÿà§‡à¦›à§‡ (Very Important)

## ðŸ”¹ model.compile()

| Parameter | à¦•à§‡à¦¨ à¦¦à¦°à¦•à¦¾à¦°                              |
| --------- | -------------------------------------- |
| optimizer | weight update à¦¨à¦¾ à¦¦à¦¿à¦²à§‡ training à¦¹à¦¬à§‡ à¦¨à¦¾  |
| loss      | error calculate à¦¨à¦¾ à¦¹à¦²à§‡ learning à¦¹à¦¬à§‡ à¦¨à¦¾ |
| metrics   | accuracy à¦¦à§‡à¦–à¦¾à¦° à¦œà¦¨à§à¦¯                    |

---

## ðŸ”¹ model.fit()

| Parameter | à¦•à§‡à¦¨ à¦¦à¦°à¦•à¦¾à¦°     |
| --------- | ------------- |
| x         | input data    |
| y         | true labels   |
| epochs    | training loop |

---

## ðŸ”¹ model.evaluate()

| Parameter | à¦•à§‡à¦¨ à¦¦à¦°à¦•à¦¾à¦°   |
| --------- | ----------- |
| x         | test input  |
| y         | true labels |

---

## ðŸ”¹ model.predict()

| Parameter | à¦•à§‡à¦¨ à¦¦à¦°à¦•à¦¾à¦°        |
| --------- | ---------------- |
| x         | prediction input |

---

# âœ… Final Output à¦¤à§à¦®à¦¿ à¦•à§€ à¦•à§€ à¦ªà¦¾à¦¬à§‡

âœ” Training loss curve
âœ” Training accuracy curve
âœ” Test accuracy
âœ” 10à¦Ÿà¦¾ digit image
âœ” à¦ªà§à¦°à¦¤à¦¿à¦Ÿà¦¾à¦° predicted digit
âœ” à¦ªà§à¦°à¦¤à¦¿à¦Ÿà¦¾à¦° max probability

à¦¸à¦¬à¦•à¦¿à¦›à§ **minimal parameters** à¦¦à¦¿à§Ÿà§‡à¦‡ âœ…

---

## ðŸ§ª Exam / Viva Ready Line

> **History object à¦¦à¦¿à§Ÿà§‡ training loss à¦“ accuracy curve à¦ªà¦¾à¦“à§Ÿà¦¾ à¦¯à¦¾à§Ÿ,
> predict() probability à¦¦à§‡à§Ÿ, argmax à¦¦à¦¿à§Ÿà§‡ class à¦¬à§‡à¦° à¦•à¦°à¦¾ à¦¹à§Ÿà¥¤**

---


