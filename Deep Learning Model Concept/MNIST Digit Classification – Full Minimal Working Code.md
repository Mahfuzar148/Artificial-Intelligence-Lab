

---

# ğŸ“˜ MNIST Digit Classification â€“ Full Minimal Working Code (Keras)

---

## ğŸ”¹ Step 1: Import Required Libraries (Minimal)

```python
import tensorflow as tf
from tensorflow.keras.datasets import mnist
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten
```

---

## ğŸ”¹ Step 2: Load MNIST Dataset

```python
(x_train, y_train), (x_test, y_test) = mnist.load_data()
```

ğŸ“Œ Dataset info:

* Image size: `28 Ã— 28`
* Classes: `0â€“9` (10 classes)

---

## ğŸ”¹ Step 3: Preprocess Data (Mandatory)

### Normalize images

```python
x_train = x_train / 255.0
x_test  = x_test / 255.0
```

ğŸ“Œ Reason:

* Pixel range `0â€“255` â†’ `0â€“1`
* Faster & stable training

---

## ğŸ”¹ Step 4: Build Model (Sequential, Minimal)

```python
model = Sequential([
    Flatten(input_shape=(28, 28)),
    Dense(32, activation='relu'),
    Dense(10, activation='softmax')
])
```

ğŸ“Œ Explanation:

* `Flatten` â†’ image â†’ 1D vector (784)
* `Dense(32)` â†’ hidden layer
* `Dense(10)` â†’ 10 digit classes

---

## ğŸ”¹ Step 5: Compile Model (MANDATORY)

```python
model.compile(
    optimizer='adam',
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)
```

ğŸ“Œ Minimum required:

* optimizer
* loss
* metrics (optional but practical)

---

## ğŸ”¹ Step 6: Train Model (fit)

```python
model.fit(
    x_train,
    y_train,
    epochs=5
)
```

ğŸ“Œ Minimum required:

* training data
* labels
* epochs

---

## ğŸ”¹ Step 7: Evaluate Model

```python
test_loss, test_accuracy = model.evaluate(x_test, y_test)
print("Test Accuracy:", test_accuracy)
```

ğŸ“Œ Purpose:

* Model à¦•à¦¤à¦Ÿà¦¾ à¦­à¦¾à¦²à§‹ à¦•à¦¾à¦œ à¦•à¦°à¦›à§‡ à¦¤à¦¾ à¦¦à§‡à¦–à¦¾

---

## ğŸ”¹ Step 8: Predict (Single Sample)

```python
predictions = model.predict(x_test)
```

### Predict one image

```python
import numpy as np

predicted_label = np.argmax(predictions[0])
true_label = y_test[0]

print("Predicted:", predicted_label)
print("Actual:", true_label)
```

ğŸ“Œ `argmax` â†’ highest probability class

---

# ğŸ§  Minimal Parameters Summary (Exam Gold â­)

| Function       | Mandatory Parameters |
| -------------- | -------------------- |
| `Sequential()` | layers               |
| `Dense()`      | units                |
| `compile()`    | optimizer, loss      |
| `fit()`        | x, y, epochs         |
| `evaluate()`   | x, y                 |
| `predict()`    | x                    |

---

# ğŸ§ª One-Block Complete Code (Copyâ€“Paste Ready)

```python
import tensorflow as tf
import numpy as np
from tensorflow.keras.datasets import mnist
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten

# Load data
(x_train, y_train), (x_test, y_test) = mnist.load_data()

# Normalize
x_train = x_train / 255.0
x_test  = x_test / 255.0

# Build model
model = Sequential([
    Flatten(input_shape=(28, 28)),
    Dense(32, activation='relu'),
    Dense(10, activation='softmax')
])

# Compile
model.compile(
    optimizer='adam',
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

# Train
model.fit(x_train, y_train, epochs=5)

# Evaluate
loss, acc = model.evaluate(x_test, y_test)
print("Test Accuracy:", acc)

# Predict
pred = model.predict(x_test)
print("Predicted label:", np.argmax(pred[0]))
print("Actual label:", y_test[0])
```

---

## âœ… Final Notes (Very Important)

* âœ” MNIST labels integer â†’ `sparse_categorical_crossentropy`
* âœ” Softmax output units = number of classes
* âœ” Sequential best for this problem
* âœ” This is **minimum working deep learning pipeline**

---

