
---

# ğŸ“˜ Full Documentation: CNN Architecture â€“ Layer by Layer Explanation

---

# ğŸ§  1ï¸âƒ£ Input Layer

## ğŸ”¹ à¦•à§€ à¦•à¦¾à¦œ à¦•à¦°à§‡?

* Raw image à¦—à§à¦°à¦¹à¦£ à¦•à¦°à§‡
* Data model-à¦ à¦ªà¦¾à¦ à¦¾à§Ÿ

## ğŸ”¹ Example

```python
Input((28, 28, 1))
```

### à¦®à¦¾à¦¨à§‡:

* Height = 28
* Width = 28
* Channel = 1 (grayscale)

---

## ğŸ”¹ Important Note

Input layer:

* à¦•à§‹à¦¨à§‹ computation à¦•à¦°à§‡ à¦¨à¦¾
* à¦•à§‹à¦¨à§‹ parameter à¦¨à§‡à¦‡

---

# ğŸ”µ 2ï¸âƒ£ Convolution Layer (Conv2D)

## ğŸ”¹ à¦®à§‚à¦² à¦•à¦¾à¦œ

Feature extraction

## ğŸ”¹ à¦•à§€à¦­à¦¾à¦¬à§‡ à¦•à¦¾à¦œ à¦•à¦°à§‡?

* Filter (kernel) image-à¦à¦° à¦‰à¦ªà¦° slide à¦•à¦°à§‡
* Element-wise multiply + sum à¦•à¦°à§‡
* Feature map à¦¤à§ˆà¦°à¦¿ à¦•à¦°à§‡

---

## ğŸ”¹ Example

```python
Conv2D(32, (3,3), activation='relu')
```

### à¦®à¦¾à¦¨à§‡:

* 32à¦Ÿà¦¾ filter
* à¦ªà§à¦°à¦¤à¦¿à¦Ÿà¦¾ filter 3Ã—3
* Output = 32 feature map

---

## ğŸ”¹ Output Shape Calculation

Input: 28Ã—28
Filter: 3Ã—3

Output:

[
(28 - 3 + 1) = 26
]

Output size = 26Ã—26Ã—32

---

## ğŸ”¹ à¦•à§‡à¦¨ à¦¦à¦°à¦•à¦¾à¦°?

* Edge detect à¦•à¦°à§‡
* Texture detect à¦•à¦°à§‡
* Pattern detect à¦•à¦°à§‡

---

# ğŸŸ¢ 3ï¸âƒ£ Activation Layer (ReLU)

## ğŸ”¹ à¦•à¦¾à¦œ

Non-linearity à¦†à¦¨à¦¾

## ğŸ”¹ Formula

[
ReLU(x) = max(0, x)
]

---

## ğŸ”¹ à¦•à§‡à¦¨ à¦¦à¦°à¦•à¦¾à¦°?

Activation à¦›à¦¾à§œà¦¾:

```
Linear â†’ Linear â†’ Linear
```

à¦ªà§à¦°à§‹ network linear à¦¹à§Ÿà§‡ à¦¯à¦¾à¦¬à§‡ âŒ

Activation network-à¦•à§‡ complex pattern à¦¶à¦¿à¦–à¦¤à§‡ à¦¸à¦¾à¦¹à¦¾à¦¯à§à¦¯ à¦•à¦°à§‡à¥¤

---

# ğŸŸ£ 4ï¸âƒ£ Pooling Layer (MaxPooling2D)

## ğŸ”¹ à¦•à¦¾à¦œ

* Feature map size à¦•à¦®à¦¾à¦¨à§‹
* Important feature à¦°à¦¾à¦–à¦¾

---

## ğŸ”¹ Example

```python
MaxPooling2D((2,2))
```

### à¦®à¦¾à¦¨à§‡:

2Ã—2 block à¦¥à§‡à¦•à§‡ maximum à¦¨à§‡à§Ÿ

---

## ğŸ”¹ à¦•à§‡à¦¨ à¦¦à¦°à¦•à¦¾à¦°?

* Computation à¦•à¦®à¦¾à§Ÿ
* Overfitting à¦•à¦®à¦¾à§Ÿ
* Translation invariance à¦¦à§‡à§Ÿ

---

# ğŸŸ¡ 5ï¸âƒ£ Dropout Layer

## ğŸ”¹ à¦•à¦¾à¦œ

Random neuron à¦¬à¦¨à§à¦§ à¦•à¦°à§‡ overfitting à¦•à¦®à¦¾à§Ÿ

---

## ğŸ”¹ Example

```python
Dropout(0.5)
```

à¦®à¦¾à¦¨à§‡:

à§«à§¦% neuron training à¦¸à¦®à§Ÿ à¦¬à¦¨à§à¦§ à¦¥à¦¾à¦•à¦¬à§‡à¥¤

---

# ğŸŸ  6ï¸âƒ£ Batch Normalization

## ğŸ”¹ à¦•à¦¾à¦œ

* Training stable à¦•à¦°à¦¾
* Faster convergence

---

## ğŸ”¹ Example

```python
BatchNormalization()
```

---

# ğŸ”µ 7ï¸âƒ£ Flatten Layer

## ğŸ”¹ à¦•à¦¾à¦œ

3D feature map â†’ 1D vector

---

## ğŸ”¹ Example

Input:

```
7 Ã— 7 Ã— 64
```

Flatten:

```
3136
```

---

## ğŸ”¹ à¦•à§‡à¦¨ à¦¦à¦°à¦•à¦¾à¦°?

Dense layer 2D input à¦šà¦¾à§Ÿà¥¤

---

# ğŸ”´ 8ï¸âƒ£ Fully Connected (Dense Layer)

## ğŸ”¹ à¦•à¦¾à¦œ

Final classification logic à¦¶à§‡à¦–à¦¾

---

## ğŸ”¹ Example

```python
Dense(128, activation='relu')
```

---

# âš« 9ï¸âƒ£ Output Layer

## ğŸ”¹ à¦•à¦¾à¦œ

Final prediction à¦¦à§‡à¦“à§Ÿà¦¾

---

## ğŸ”¹ Case à¦…à¦¨à§à¦¯à¦¾à§Ÿà§€

### Binary:

```python
Dense(1, activation='sigmoid')
```

### Multi-class:

```python
Dense(10, activation='softmax')
```

---

# ğŸ§  Complete CNN Flow

```
Input
â†“
Conv â†’ ReLU
â†“
Pooling
â†“
Conv â†’ ReLU
â†“
Pooling
â†“
Flatten
â†“
Dense
â†“
Output
```

---

# ğŸ“Š Example Full CNN Architecture

```python
inputs = Input((28,28,1))

x = Conv2D(32, (3,3), activation='relu')(inputs)
x = MaxPooling2D((2,2))(x)

x = Conv2D(64, (3,3), activation='relu')(x)
x = MaxPooling2D((2,2))(x)

x = Flatten()(x)
x = Dense(128, activation='relu')(x)

outputs = Dense(10, activation='softmax')(x)

model = Model(inputs, outputs)
```

---

# ğŸ¯ CNN Architecture à¦•à§‡à¦¨ à¦¶à¦•à§à¦¤à¦¿à¦¶à¦¾à¦²à§€?

| Layer   | à¦•à¦¾à¦œ                |
| ------- | ------------------ |
| Conv    | Feature extraction |
| Pool    | Compression        |
| Flatten | Reshape            |
| Dense   | Decision making    |

---

# ğŸ§  CNN vs FCNN à¦ªà¦¾à¦°à§à¦¥à¦•à§à¦¯

| Feature              | CNN    | FCNN   |
| -------------------- | ------ | ------ |
| Spatial awareness    | âœ… à¦†à¦›à§‡  | âŒ à¦¨à§‡à¦‡  |
| Image performance    | High   | Medium |
| Parameter efficiency | Better | Heavy  |

---

# ğŸ“ Viva Ready Summary

> CNN architecture sequential à¦­à¦¾à¦¬à§‡ feature extraction à¦¥à§‡à¦•à§‡ classification à¦ªà¦°à§à¦¯à¦¨à§à¦¤ à¦•à¦¾à¦œ à¦•à¦°à§‡à¥¤ Convolution layer feature à¦¬à§‡à¦° à¦•à¦°à§‡, pooling layer size à¦•à¦®à¦¾à§Ÿ, flatten layer data reshape à¦•à¦°à§‡ à¦à¦¬à¦‚ dense layer final classification à¦•à¦°à§‡à¥¤

---

# ğŸš€ Extra Deep Insight

CNN à¦§à¦¾à¦ªà§‡ à¦§à¦¾à¦ªà§‡ à¦¶à§‡à¦–à§‡:

* à¦ªà§à¦°à¦¥à¦® layer â†’ Edge
* à¦®à¦¾à¦à§‡à¦° layer â†’ Shape
* à¦¶à§‡à¦· layer â†’ Object

---


1. Basic CNN
2. LeNet-5
3. AlexNet
4. VGG16-style
5. ResNet (Residual Block à¦¸à¦¹)
6. MobileNet (Depthwise Separable Conv)
7. U-Net (Segmentation)
8. 1D CNN
9. 3D CNN


* ğŸ”¹ Concept
* ğŸ”¹ Architecture explanation
* ğŸ”¹ When to use
* ğŸ”¹ Full model code

---

# ğŸŸ¢ 1ï¸âƒ£ Basic CNN

## ğŸ”¹ Use Case

* MNIST
* Small dataset

## ğŸ”¹ Architecture

Conv â†’ Pool â†’ Conv â†’ Pool â†’ Flatten â†’ Dense â†’ Output

## ğŸ”¹ Full Code

```python
import tensorflow as tf
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Input
from tensorflow.keras.models import Model

inputs = Input((28,28,1))

x = Conv2D(32, (3,3), activation='relu')(inputs)
x = MaxPooling2D((2,2))(x)

x = Conv2D(64, (3,3), activation='relu')(x)
x = MaxPooling2D((2,2))(x)

x = Flatten()(x)
x = Dense(128, activation='relu')(x)

outputs = Dense(10, activation='softmax')(x)

model = Model(inputs, outputs)
model.summary()
```

---

# ğŸ”µ 2ï¸âƒ£ LeNet-5

## ğŸ”¹ Developed by: Yann LeCun

## ğŸ”¹ Used for: Digit recognition

## ğŸ”¹ Architecture

Conv(6) â†’ Pool â†’ Conv(16) â†’ Pool â†’ FC â†’ FC â†’ Output

## ğŸ”¹ Full Code

```python
inputs = Input((32,32,1))

x = Conv2D(6, (5,5), activation='relu')(inputs)
x = MaxPooling2D((2,2))(x)

x = Conv2D(16, (5,5), activation='relu')(x)
x = MaxPooling2D((2,2))(x)

x = Flatten()(x)
x = Dense(120, activation='relu')(x)
x = Dense(84, activation='relu')(x)

outputs = Dense(10, activation='softmax')(x)

model = Model(inputs, outputs)
model.summary()
```

---

# ğŸ”´ 3ï¸âƒ£ AlexNet

## ğŸ”¹ ImageNet 2012 Winner

## ğŸ”¹ Key Features

* Large filters
* ReLU
* Dropout

## ğŸ”¹ Full Code (Simplified)

```python
from tensorflow.keras.layers import Dropout

inputs = Input((227,227,3))

x = Conv2D(96, (11,11), strides=4, activation='relu')(inputs)
x = MaxPooling2D((3,3), strides=2)(x)

x = Conv2D(256, (5,5), activation='relu')(x)
x = MaxPooling2D((3,3), strides=2)(x)

x = Conv2D(384, (3,3), activation='relu')(x)
x = Conv2D(384, (3,3), activation='relu')(x)
x = Conv2D(256, (3,3), activation='relu')(x)
x = MaxPooling2D((3,3), strides=2)(x)

x = Flatten()(x)
x = Dense(4096, activation='relu')(x)
x = Dropout(0.5)(x)
x = Dense(4096, activation='relu')(x)
x = Dropout(0.5)(x)

outputs = Dense(1000, activation='softmax')(x)

model = Model(inputs, outputs)
model.summary()
```

---

# ğŸŸ£ 4ï¸âƒ£ VGG16

## ğŸ”¹ Key Idea

* Multiple 3Ã—3 Conv
* Very deep

## ğŸ”¹ Full Code (VGG Block Style)

```python
inputs = Input((224,224,3))

x = Conv2D(64, (3,3), activation='relu', padding='same')(inputs)
x = Conv2D(64, (3,3), activation='relu', padding='same')(x)
x = MaxPooling2D((2,2))(x)

x = Conv2D(128, (3,3), activation='relu', padding='same')(x)
x = Conv2D(128, (3,3), activation='relu', padding='same')(x)
x = MaxPooling2D((2,2))(x)

x = Flatten()(x)
x = Dense(4096, activation='relu')(x)
x = Dense(4096, activation='relu')(x)

outputs = Dense(1000, activation='softmax')(x)

model = Model(inputs, outputs)
model.summary()
```

---

# ğŸŸ¤ 5ï¸âƒ£ ResNet (With Residual Block)

## ğŸ”¹ Key Innovation

Skip connection: F(x) + x

## ğŸ”¹ Residual Block Code

```python
from tensorflow.keras.layers import Add

def residual_block(x, filters):
    shortcut = x
    
    x = Conv2D(filters, (3,3), padding='same', activation='relu')(x)
    x = Conv2D(filters, (3,3), padding='same')(x)
    
    x = Add()([x, shortcut])
    x = tf.keras.activations.relu(x)
    
    return x

inputs = Input((64,64,3))
x = Conv2D(64, (3,3), padding='same')(inputs)

x = residual_block(x, 64)
x = residual_block(x, 64)

x = Flatten()(x)
outputs = Dense(10, activation='softmax')(x)

model = Model(inputs, outputs)
model.summary()
```

---

# âš« 6ï¸âƒ£ MobileNet (Depthwise Separable Conv)

```python
from tensorflow.keras.layers import DepthwiseConv2D

inputs = Input((128,128,3))

x = DepthwiseConv2D((3,3), padding='same', activation='relu')(inputs)
x = Conv2D(64, (1,1), activation='relu')(x)

x = DepthwiseConv2D((3,3), padding='same', activation='relu')(x)
x = Conv2D(128, (1,1), activation='relu')(x)

x = Flatten()(x)
outputs = Dense(10, activation='softmax')(x)

model = Model(inputs, outputs)
model.summary()
```

---

# ğŸŸ¢ 7ï¸âƒ£ U-Net (Segmentation)

```python
from tensorflow.keras.layers import UpSampling2D, Concatenate

inputs = Input((128,128,1))

# Encoder
c1 = Conv2D(64, (3,3), activation='relu', padding='same')(inputs)
p1 = MaxPooling2D((2,2))(c1)

# Bottleneck
b = Conv2D(128, (3,3), activation='relu', padding='same')(p1)

# Decoder
u1 = UpSampling2D((2,2))(b)
concat = Concatenate()([u1, c1])
c2 = Conv2D(64, (3,3), activation='relu', padding='same')(concat)

outputs = Conv2D(1, (1,1), activation='sigmoid')(c2)

model = Model(inputs, outputs)
model.summary()
```

---

# ğŸ”µ 8ï¸âƒ£ 1D CNN (Time Series)

```python
from tensorflow.keras.layers import Conv1D, MaxPooling1D

inputs = Input((100,1))

x = Conv1D(32, 3, activation='relu')(inputs)
x = MaxPooling1D(2)(x)

x = Flatten()(x)
outputs = Dense(1, activation='sigmoid')(x)

model = Model(inputs, outputs)
model.summary()
```

---

# ğŸŸ¡ 9ï¸âƒ£ 3D CNN (Video)

```python
from tensorflow.keras.layers import Conv3D, MaxPooling3D

inputs = Input((16,64,64,3))

x = Conv3D(32, (3,3,3), activation='relu')(inputs)
x = MaxPooling3D((2,2,2))(x)

x = Flatten()(x)
outputs = Dense(10, activation='softmax')(x)

model = Model(inputs, outputs)
model.summary()
```

---

# ğŸ¯ Final Understanding

| Model     | Main Idea              | Use Case             |
| --------- | ---------------------- | -------------------- |
| Basic CNN | Simple conv            | Small dataset        |
| LeNet     | Early digit model      | MNIST                |
| AlexNet   | Deep + dropout         | ImageNet             |
| VGG       | Small filters deep net | Large classification |
| ResNet    | Skip connection        | Very deep network    |
| MobileNet | Lightweight            | Mobile               |
| U-Net     | Encoder-decoder        | Segmentation         |
| 1D CNN    | Sequence data          | Time series          |
| 3D CNN    | Video                  | Action recognition   |

---

