
---

# ðŸ§¾ Optimizer â€” Full Documentation (TensorFlow / Keras)

---

## ðŸ”¹ Optimizer à¦•à§€?

ðŸ‘‰ **Optimizer** à¦¹à¦²à§‹ à¦¸à§‡à¦‡ algorithm à¦¯à¦¾ à¦¬à¦²à§‡ à¦¦à§‡à§Ÿ:

> **loss à¦•à¦®à¦¾à¦¨à§‹à¦° à¦œà¦¨à§à¦¯ model-à¦à¦° weight à¦•à§€à¦­à¦¾à¦¬à§‡ update à¦¹à¦¬à§‡**

à¦¸à¦¹à¦œà¦­à¦¾à¦¬à§‡:

```
Loss â†’ Gradient â†’ Optimizer â†’ Weight update
```

ðŸ“Œ Optimizer à¦›à¦¾à§œà¦¾ neural network **à¦¶à¦¿à¦–à¦¤à§‡à¦‡ à¦ªà¦¾à¦°à¦¬à§‡ à¦¨à¦¾**à¥¤

---

## ðŸ”¹ Optimizer à¦•à§€ à¦•à¦¾à¦œ à¦•à¦°à§‡?

à¦ªà§à¦°à¦¤à¦¿à¦Ÿà¦¾ training step-à¦ optimizer:

1ï¸âƒ£ Loss calculate à¦•à¦°à§‡
2ï¸âƒ£ Gradient à¦¬à§‡à¦° à¦•à¦°à§‡ (`âˆ‚loss/âˆ‚weight`)
3ï¸âƒ£ Weight update à¦•à¦°à§‡

Formula (basic idea):

```
new_weight = old_weight âˆ’ learning_rate Ã— gradient
```

---

## ðŸ”¹ `model.compile()`-à¦ optimizer à¦•à§€à¦­à¦¾à¦¬à§‡ à¦¦à§‡à¦“à§Ÿà¦¾ à¦¹à§Ÿ?

### â–¶ï¸ String form (simple)

```python
model.compile(
    optimizer='adam',
    loss='mse'
)
```

### â–¶ï¸ Object form (recommended)

```python
from tensorflow.keras.optimizers import Adam

optimizer = Adam(learning_rate=0.001)

model.compile(
    optimizer=optimizer,
    loss='mse'
)
```

---

# ðŸ”´ Core Parameter (à¦¸à¦¬ optimizer-à¦ common)

## ðŸ”‘ `learning_rate` (à¦¸à¦¬à¦šà§‡à§Ÿà§‡ à¦—à§à¦°à§à¦¤à§à¦¬à¦ªà§‚à¦°à§à¦£)

### ðŸ”¹ learning rate à¦•à§€?

ðŸ‘‰ weight à¦•à¦¤ **à¦¦à§‚à¦°à§‡ à¦¯à¦¾à¦¬à§‡** à¦¸à§‡à¦Ÿà¦¾ à¦ à¦¿à¦• à¦•à¦°à§‡

* à¦›à§‹à¦Ÿ â†’ slow learning
* à¦¬à§œ â†’ unstable / diverge

```python
Adam(learning_rate=0.001)
```

### ðŸ”¹ Wrong learning rate à¦¹à¦²à§‡ à¦•à§€ à¦¹à§Ÿ?

| LR      | Result             |
| ------- | ------------------ |
| à¦–à§à¦¬ à¦¬à§œ  | Loss explode       |
| à¦–à§à¦¬ à¦›à§‹à¦Ÿ | Training à¦–à§à¦¬ slow  |
| à¦ à¦¿à¦•     | Smooth convergence |

---

# ðŸ”¹ Common Optimizers (à¦¸à¦¬à¦šà§‡à§Ÿà§‡ à¦¬à§‡à¦¶à¦¿ à¦¬à§à¦¯à¦¬à¦¹à§ƒà¦¤)

---

## 1ï¸âƒ£ SGD (Stochastic Gradient Descent)

### ðŸ”¹ Concept

à¦¸à¦¬à¦šà§‡à§Ÿà§‡ basic optimizer

```python
from tensorflow.keras.optimizers import SGD

optimizer = SGD(
    learning_rate=0.01,
    momentum=0.9,
    nesterov=False
)
```

### ðŸ”¹ Parameters

| Parameter       | à¦•à¦¾à¦œ                  |
| --------------- | -------------------- |
| `learning_rate` | Step size            |
| `momentum`      | Past gradient memory |
| `nesterov`      | Advanced momentum    |

### ðŸ”¹ Use-case

* Simple problems
* When you want full control

---

## 2ï¸âƒ£ Adam (MOST POPULAR)

### ðŸ”¹ Concept

SGD + Momentum + RMSProp

```python
from tensorflow.keras.optimizers import Adam

optimizer = Adam(
    learning_rate=0.001,
    beta_1=0.9,
    beta_2=0.999,
    epsilon=1e-7
)
```

### ðŸ”¹ Parameters

| Parameter       | à¦•à¦¾à¦œ                 |
| --------------- | ------------------- |
| `learning_rate` | Main step size      |
| `beta_1`        | 1st moment decay    |
| `beta_2`        | 2nd moment decay    |
| `epsilon`       | Numerical stability |

### ðŸ”¹ Why Adam is popular?

âœ” Fast convergence
âœ” Less tuning
âœ” Default choice

---

## 3ï¸âƒ£ RMSprop

### ðŸ”¹ Concept

Adaptive learning rate per parameter

```python
from tensorflow.keras.optimizers import RMSprop

optimizer = RMSprop(
    learning_rate=0.001,
    rho=0.9,
    epsilon=1e-7
)
```

### ðŸ”¹ Use-case

* RNN
* Sequence data

---

## 4ï¸âƒ£ Adagrad

### ðŸ”¹ Concept

Learning rate decreases over time

```python
from tensorflow.keras.optimizers import Adagrad

optimizer = Adagrad(
    learning_rate=0.01,
    initial_accumulator_value=0.1
)
```

### ðŸ”¹ Problem

* LR à¦–à§à¦¬ à¦¦à§à¦°à§à¦¤ à¦›à§‹à¦Ÿ à¦¹à§Ÿà§‡ à¦¯à¦¾à§Ÿ

---

## 5ï¸âƒ£ Adamax

### ðŸ”¹ Concept

Adam-à¦à¦° infinity-norm version

```python
from tensorflow.keras.optimizers import Adamax(
    learning_rate=0.002
)
```

---

## 6ï¸âƒ£ Nadam

### ðŸ”¹ Concept

Adam + Nesterov momentum

```python
from tensorflow.keras.optimizers import Nadam(
    learning_rate=0.002
)
```

---

# ðŸ”¹ Optimizer Parameters (Common Summary)

| Parameter       | Meaning             |
| --------------- | ------------------- |
| `learning_rate` | Step size           |
| `momentum`      | Gradient memory     |
| `beta_1`        | First moment decay  |
| `beta_2`        | Second moment decay |
| `epsilon`       | Numerical stability |
| `weight_decay`  | Regularization      |

---

# ðŸ”¹ Optimizer vs Loss vs Metrics (Confusion Clear)

| Term      | Role               |
| --------- | ------------------ |
| Optimizer | Weight update      |
| Loss      | Error measure      |
| Metrics   | Performance report |

ðŸ“Œ **Optimizer uses loss, metrics does not affect training**

---

# ðŸ”¹ Learning Rate Scheduler (Advanced)

```python
from tensorflow.keras.optimizers.schedules import ExponentialDecay

lr_schedule = ExponentialDecay(
    initial_learning_rate=0.01,
    decay_steps=1000,
    decay_rate=0.96
)

optimizer = Adam(learning_rate=lr_schedule)
```

---

# ðŸ”¹ When to use which optimizer?

| Scenario           | Optimizer           |
| ------------------ | ------------------- |
| Beginner / general | Adam                |
| Fine control       | SGD                 |
| RNN                | RMSprop             |
| Sparse data        | Adagrad             |
| Transfer learning  | Adam / SGD (low LR) |

---

# âš ï¸ Common Mistakes (VERY IMPORTANT)

âŒ Learning rate à¦–à§à¦¬ à¦¬à§œ
âŒ Optimizer change à¦•à¦°à§‡ recompile à¦¨à¦¾ à¦•à¦°à¦¾
âŒ Default Adam blindly à¦¸à¦¬ à¦œà¦¾à§Ÿà¦—à¦¾à§Ÿ
âŒ Freeze layer à¦•à¦¿à¦¨à§à¦¤à§ optimizer change à¦¨à¦¾ à¦•à¦°à¦¾

---

# ðŸ§  Interview-ready One-liners

* Optimizer controls how weights are updated
* Learning rate is the most critical hyperparameter
* Adam is adaptive and widely used
* Optimizer minimizes the loss function

---

# ðŸ“Œ Minimal Working Example

```python
model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
    loss='mse',
    metrics=['mae']
)
```

---

## ðŸ Final Takeaway

> **Optimizer is the engine of learning â€” learning rate is its accelerator.**

---

