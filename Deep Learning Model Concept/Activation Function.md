
---

# üìò Activation Function ‚Äì Full Documentation (Deep Learning)

This document provides a **complete, exam-ready and beginner-friendly guide** to **Activation Functions** used in Deep Learning and Neural Networks.

---

## üìå Table of Contents

1. What is an Activation Function?
2. Why Activation Function is Needed
3. Neuron With vs Without Activation
4. Where Activation Function is Used
5. Types of Activation Functions
6. Activation Selection Rules
7. Common Mistakes
8. Example Model
9. Cheat Sheet
10. Final Summary

---

## üîπ 1. What is an Activation Function?

An **Activation Function** is a mathematical function that:

* Introduces **non-linearity** into a neural network
* Enables the network to learn **complex patterns and relationships**

üìå Without activation functions, a neural network behaves like a **simple linear model**, no matter how deep it is.

---

## üîπ 2. Neuron Without vs With Activation

### ‚ùå Without Activation

```
y = Wx + b
```

* Linear model
* Deep network behaves like a shallow network
* Cannot solve non-linear problems (e.g. XOR)

---

### ‚úÖ With Activation

```
y = f(Wx + b)
```

* Non-linear model
* Can learn complex real-world patterns
* Enables deep learning

---

## üîπ 3. Where Activation Function is Used?

```
Input ‚Üí Dense ‚Üí Activation ‚Üí Output
```

or directly inside a layer:

```python
Dense(64, activation='relu')
```

---

## üîπ 4. Why Activation Function is Needed?

| Without Activation | With Activation |
| ------------------ | --------------- |
| Only linear        | Non-linear      |
| XOR ‚ùå              | XOR ‚úÖ           |
| Deep = useless     | Deep = powerful |

---

# üî• 5. Types of Activation Functions

---

## 1Ô∏è‚É£ Linear (No Activation)

### Formula

```
f(x) = x
```

### Keras Example

```python
Dense(1, activation='linear')
```

### Use Cases

* Regression
* Price, temperature prediction

‚ö†Ô∏è Not used in hidden layers

---

## 2Ô∏è‚É£ Sigmoid

### Formula

```
f(x) = 1 / (1 + e^(-x))
```

### Output Range

```
(0, 1)
```

### Keras Example

```python
Dense(1, activation='sigmoid')
```

### Use Cases

* Binary classification
* Probability output

‚ùå Problems:

* Vanishing gradient
* Slow training

---

## 3Ô∏è‚É£ Tanh

### Formula

```
f(x) = tanh(x)
```

### Output Range

```
(-1, 1)
```

### Keras Example

```python
Dense(64, activation='tanh')
```

### Use Cases

* Hidden layers (older models)
* RNNs

‚ùå Problem:

* Vanishing gradient

---

## 4Ô∏è‚É£ ReLU ‚≠ê (Most Popular)

### Formula

```
f(x) = max(0, x)
```

### Output Range

```
[0, ‚àû)
```

### Keras Example

```python
Dense(64, activation='relu')
```

### Use Cases

* Hidden layers
* CNN / DNN

‚ùå Problem:

* Dead neuron problem

---

## 5Ô∏è‚É£ Leaky ReLU

### Formula

```
f(x) = x        if x > 0
f(x) = Œ±x       if x ‚â§ 0
```

### Keras Example

```python
from tensorflow.keras.layers import LeakyReLU
LeakyReLU(alpha=0.1)
```

### Use Case

* Solves ReLU dead neuron problem

---

## 6Ô∏è‚É£ PReLU

### Key Idea

* Œ± is **learnable**

### Keras Example

```python
from tensorflow.keras.layers import PReLU
PReLU()
```

### Use Case

* Adaptive slope learning

---

## 7Ô∏è‚É£ ELU

### Feature

* Smooth negative output

### Keras Example

```python
Dense(64, activation='elu')
```

### Use Case

* Faster convergence (some cases)

---

## 8Ô∏è‚É£ Softmax ‚≠ê

### Formula

```
f(x_i) = exp(x_i) / Œ£ exp(x_j)
```

### Property

```
Sum of outputs = 1
```

### Keras Example

```python
Dense(10, activation='softmax')
```

### Use Case

* Multi-class classification (output layer only)

---

## 9Ô∏è‚É£ Swish

### Formula

```
f(x) = x * sigmoid(x)
```

### Keras Example

```python
Dense(64, activation='swish')
```

### Use Case

* Modern deep networks
* EfficientNet

---

## üîü GELU

### Feature

* Gaussian-based activation

### Keras Example

```python
Dense(64, activation='gelu')
```

### Use Case

* Transformers
* NLP models (BERT)

---

## üß† 6. Activation Selection Rules (Exam Important)

| Layer Type         | Best Activation |
| ------------------ | --------------- |
| Hidden layer       | ReLU            |
| Binary output      | Sigmoid         |
| Multi-class output | Softmax         |
| Regression output  | Linear          |
| RNN                | Tanh            |
| Transformer        | GELU            |

---

## ‚ùå 7. Common Mistakes

### ‚ùå Softmax in Hidden Layer

```python
Dense(64, activation='softmax')  # WRONG
```

### ‚ùå Sigmoid for Multi-class

```python
Dense(3, activation='sigmoid')   # WRONG
```

### ‚ùå No Activation

```python
Dense(64)  # Weak model
```

---

## üß™ 8. Example Model Using Activations

```python
from tensorflow.keras.layers import Input, Dense
from tensorflow.keras.models import Model

inputs = Input((10,))
x = Dense(64, activation='relu')(inputs)
x = Dense(32, activation='relu')(x)
outputs = Dense(1, activation='sigmoid')(x)

model = Model(inputs, outputs)
model.summary()
```

---

## üìå 9. Activation Cheat Sheet

| Task                       | Activation |
| -------------------------- | ---------- |
| Regression                 | Linear     |
| Binary Classification      | Sigmoid    |
| Multi-class Classification | Softmax    |
| Hidden Layers              | ReLU       |
| NLP / Transformers         | GELU       |

---

## ‚úÖ 10. Final Summary

* Activation functions add **non-linearity**
* Without activation, deep networks are useless
* ReLU is the default for hidden layers
* Output activation depends on problem type

---

üìå **Next Possible Extensions**

* Activation vs Loss Function Mapping
* Graphical Visualization
* Interview Questions & Answers
* Practice Problems

---

