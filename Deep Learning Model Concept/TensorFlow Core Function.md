

---

# ðŸ“˜ Keras / TensorFlow Core Function â€“ Full Detailed Documentation

à¦†à¦®à¦°à¦¾ à¦¯à§‡à¦¸à¦¬ function cover à¦•à¦°à¦¬à§‹:

1. `load_data()`
2. `Sequential()`
3. `Dense()`
4. `Flatten()`
5. `model.compile()`
6. `model.fit()`
7. `model.evaluate()`
8. `model.predict()`
9. `model.summary()`

---

## ðŸ”¹ 1. `mnist.load_data()`

### ðŸ“Œ Purpose

ðŸ‘‰ Dataset load à¦•à¦°à¦¾à¦° à¦œà¦¨à§à¦¯
ðŸ‘‰ MNIST digit data automatically download à¦•à¦°à§‡

---

### âœ… Syntax

```python
(x_train, y_train), (x_test, y_test) = mnist.load_data()
```

---

### ðŸ”¸ Parameters

```
None
```

ðŸ“Œ à¦•à§‹à¦¨à§‹ parameter à¦²à¦¾à¦—à§‡ à¦¨à¦¾

---

### ðŸ” Returns (à¦–à§à¦¬ à¦—à§à¦°à§à¦¤à§à¦¬à¦ªà§‚à¦°à§à¦£)

```
(x_train, y_train), (x_test, y_test)
```

| Variable | Type        | Shape           | Meaning         |
| -------- | ----------- | --------------- | --------------- |
| x_train  | numpy array | (60000, 28, 28) | Training images |
| y_train  | numpy array | (60000,)        | Training labels |
| x_test   | numpy array | (10000, 28, 28) | Test images     |
| y_test   | numpy array | (10000,)        | Test labels     |

---

### ðŸ“Œ à¦•à§‹à¦¥à¦¾à§Ÿ à¦¬à§à¦¯à¦¬à¦¹à¦¾à¦° à¦¹à¦¬à§‡?

* Training data à¦¹à¦¿à¦¸à§‡à¦¬à§‡ â†’ `fit()`
* Test data à¦¹à¦¿à¦¸à§‡à¦¬à§‡ â†’ `evaluate()`, `predict()`

---

## ðŸ”¹ 2. `Sequential()`

### ðŸ“Œ Purpose

ðŸ‘‰ Neural Network model à¦¬à¦¾à¦¨à¦¾à¦¨à§‹à¦° à¦œà¦¨à§à¦¯
ðŸ‘‰ Layers à¦—à§à¦²à§‹ **à¦à¦•à¦Ÿà¦¾à¦° à¦ªà¦° à¦à¦•à¦Ÿà¦¾** à¦¬à¦¸à¦¾à¦¤à§‡

---

### âœ… Syntax

```python
model = Sequential(layers)
```

---

### ðŸ”¸ Parameters (Minimal)

| Parameter | Type | Meaning                              |
| --------- | ---- | ------------------------------------ |
| layers    | list | Dense / Flatten / Conv layer à¦à¦° list |

---

### ðŸ” Returns

```
model object
```

à¦à¦‡ `model` object à¦¦à¦¿à§Ÿà§‡à¦‡:

* `compile()`
* `fit()`
* `evaluate()`
* `predict()`
  à¦¸à¦¬ à¦•à¦¾à¦œ à¦¹à§Ÿ

---

### ðŸ“Œ à¦•à§‹à¦¥à¦¾à§Ÿ à¦¬à§à¦¯à¦¬à¦¹à¦¾à¦° à¦¹à¦¬à§‡?

* Simple model
* Single input â†’ single output
* No branching

---

## ðŸ”¹ 3. `Dense()`

### ðŸ“Œ Purpose

ðŸ‘‰ Fully Connected layer à¦¬à¦¾à¦¨à¦¾à¦¨à§‹à¦° à¦œà¦¨à§à¦¯
ðŸ‘‰ Feature combine à¦•à¦°à§‡ decision à¦¨à§‡à§Ÿ

---

### âœ… Syntax

```python
Dense(units, activation=None)
```

---

### ðŸ”¸ Parameters (Minimal + Important)

| Parameter  | Mandatory | Value                        |
| ---------- | --------- | ---------------------------- |
| units      | âœ…         | Output neuron à¦¸à¦‚à¦–à§à¦¯à¦¾         |
| activation | âŒ         | 'relu', 'sigmoid', 'softmax' |

---

### ðŸ” Returns

```
Dense layer object
```

---

### ðŸ“Œ à¦•à§‹à¦¨ à¦•à§à¦·à§‡à¦¤à§à¦°à§‡ à¦•à§‹à¦¨ value?

| Case         | units          | activation    |
| ------------ | -------------- | ------------- |
| Regression   | 1              | linear / None |
| Binary class | 1              | sigmoid       |
| Multi-class  | no. of classes | softmax       |
| Hidden layer | any            | relu          |

---

## ðŸ”¹ 4. `Flatten()`

### ðŸ“Œ Purpose

ðŸ‘‰ Image / multi-dimensional data â†’ 1D vector
ðŸ‘‰ Dense layer-à¦à¦° à¦†à¦—à§‡ à¦²à¦¾à¦—à¦¬à§‡

---

### âœ… Syntax

```python
Flatten()
```

---

### ðŸ”¸ Parameters

```
None
```

---

### ðŸ” Returns

```
Flatten layer object
```

---

### ðŸ“Œ à¦•à§‹à¦¥à¦¾à§Ÿ à¦¬à§à¦¯à¦¬à¦¹à¦¾à¦° à¦¹à¦¬à§‡?

* Image data (28Ã—28)
* CNN output â†’ Dense

---

## ðŸ”¹ 5. `model.compile()`

### ðŸ“Œ Purpose

ðŸ‘‰ Model-à¦•à§‡ training-à¦à¦° à¦œà¦¨à§à¦¯ à¦ªà§à¦°à¦¸à§à¦¤à§à¦¤ à¦•à¦°à¦¾

---

### âœ… Syntax (Minimal)

```python
model.compile(optimizer, loss)
```

---

### ðŸ”¸ Parameters (Mandatory)

| Parameter | Meaning            | Example                           |
| --------- | ------------------ | --------------------------------- |
| optimizer | Weight update rule | 'adam'                            |
| loss      | Error calculation  | 'sparse_categorical_crossentropy' |

---

### ðŸ”¸ Optional (But common)

```python
metrics=['accuracy']
```

---

### ðŸ” Returns

```
None
```

ðŸ“Œ à¦•à¦¿à¦¨à§à¦¤à§ internal state à¦¤à§ˆà¦°à¦¿ à¦¹à§Ÿ

---

### ðŸ“Œ à¦•à§‹à¦¨ à¦•à§à¦·à§‡à¦¤à§à¦°à§‡ à¦•à§‹à¦¨ loss?

| Problem        | Loss                            |
| -------------- | ------------------------------- |
| Integer labels | sparse_categorical_crossentropy |
| One-hot labels | categorical_crossentropy        |
| Binary         | binary_crossentropy             |
| Regression     | mean_squared_error              |

---

## ðŸ”¹ 6. `model.fit()`

### ðŸ“Œ Purpose

ðŸ‘‰ Model training à¦¶à§à¦°à§ à¦•à¦°à¦¾

---

### âœ… Syntax (Minimal)

```python
model.fit(x, y, epochs)
```

---

### ðŸ”¸ Parameters

| Parameter | Mandatory | Meaning                 |
| --------- | --------- | ----------------------- |
| x         | âœ…         | Training data           |
| y         | âœ…         | Training labels         |
| epochs    | âœ…         | à¦•à§Ÿà¦¬à¦¾à¦° dataset train à¦¹à¦¬à§‡ |

---

### ðŸ” Returns

```
History object
```

ðŸ“Œ History object à¦¦à¦¿à§Ÿà§‡:

```python
history.history['loss']
```

loss / accuracy plot à¦•à¦°à¦¾ à¦¯à¦¾à§Ÿ

---

## ðŸ”¹ 7. `model.evaluate()`

### ðŸ“Œ Purpose

ðŸ‘‰ Trained model à¦•à¦¤à¦Ÿà¦¾ à¦­à¦¾à¦²à§‹ à¦•à¦¾à¦œ à¦•à¦°à¦›à§‡ à¦¸à§‡à¦Ÿà¦¾ à¦®à¦¾à¦ªà¦¾

---

### âœ… Syntax

```python
model.evaluate(x, y)
```

---

### ðŸ”¸ Parameters

| Parameter | Meaning     |
| --------- | ----------- |
| x         | Test data   |
| y         | True labels |

---

### ðŸ” Returns

```
loss, metrics
```

Example:

```python
loss, acc = model.evaluate(x_test, y_test)
```

---

## ðŸ”¹ 8. `model.predict()`

### ðŸ“Œ Purpose

ðŸ‘‰ Model à¦¦à¦¿à§Ÿà§‡ prediction à¦¬à§‡à¦° à¦•à¦°à¦¾

---

### âœ… Syntax

```python
model.predict(x)
```

---

### ðŸ”¸ Parameters

| Parameter | Meaning    |
| --------- | ---------- |
| x         | Input data |

---

### ðŸ” Returns

| Task        | Return             |
| ----------- | ------------------ |
| Regression  | predicted value    |
| Binary      | probability        |
| Multi-class | probability vector |

Example:

```python
pred = model.predict(x_test)
argmax(pred[i]) â†’ predicted class
```

---

## ðŸ”¹ 9. `model.summary()`

### ðŸ“Œ Purpose

ðŸ‘‰ Model architecture à¦¦à§‡à¦–à¦¾à¦¨à§‹

---

### âœ… Syntax

```python
model.summary()
```

---

### ðŸ”¸ Parameters

```
None
```

---

### ðŸ” Returns

```
None (prints table)
```

Shows:

* Layer name
* Output shape
* Parameter count

---

# ðŸ§  Master Summary Table â­

| Function   | Takes           | Returns      | Used For         |
| ---------- | --------------- | ------------ | ---------------- |
| load_data  | None            | data         | Dataset          |
| Sequential | layers          | model        | Model creation   |
| Dense      | units           | layer        | FC layer         |
| Flatten    | None            | layer        | Shape change     |
| compile    | optimizer, loss | None         | Prepare training |
| fit        | x, y, epochs    | History      | Training         |
| evaluate   | x, y            | loss, metric | Testing          |
| predict    | x               | prediction   | Inference        |
| summary    | None            | None         | Architecture     |

---

## âœ… Final Takeaway (Exam Line)

> **Deep Learning workflow:
> load â†’ preprocess â†’ build â†’ compile â†’ fit â†’ evaluate â†’ predict**

---

à¦ à¦¿à¦• à¦†à¦›à§‡ ðŸ‘
à¦à¦¬à¦¾à¦° à¦†à¦®à¦¿ à¦†à¦—à§‡à¦° à¦¡à¦•à§à¦®à§‡à¦¨à§à¦Ÿà§‡à¦¶à¦¨à¦Ÿà¦¾ **à¦†à¦°à¦“ à¦—à¦­à§€à¦°à§‡ à¦—à¦¿à§Ÿà§‡** à¦¬à§à¦¯à¦¾à¦–à§à¦¯à¦¾ à¦•à¦°à¦›à¦¿ à¦à¦¬à¦‚ **à¦ªà§à¦°à¦¤à¦¿à¦Ÿà¦¾ function-à¦à¦° à¦œà¦¨à§à¦¯ à¦†à¦²à¦¾à¦¦à¦¾ à¦†à¦²à¦¾à¦¦à¦¾ code example** à¦¦à¦¿à¦šà§à¦›à¦¿, à¦¯à¦¾à¦¤à§‡ à¦¤à§à¦®à¦¿â€”

* ðŸ”¹ concept à¦¬à§à¦à§‹
* ðŸ”¹ exam/viva-à¦¤à§‡ explain à¦•à¦°à¦¤à§‡ à¦ªà¦¾à¦°à§‹
* ðŸ”¹ à¦¨à¦¿à¦œà§‡ code à¦²à¦¿à¦–à¦¤à§‡ à¦ªà¦¾à¦°à§‹ (without confusion)

à¦†à¦®à¦¿ à¦à¦Ÿà¦¾à¦•à§‡ **step-by-step pipeline + function-wise deep explanation** à¦¹à¦¿à¦¸à§‡à¦¬à§‡ à¦¸à¦¾à¦œà¦¾à¦²à¦¾à¦®à¥¤

---

# ðŸ“˜ Keras Deep Learning Functions

## (More Explanation + Code Examples)

---

# ðŸ”¹ 1. `mnist.load_data()`

## ðŸ‘‰ à¦•à§€ à¦•à¦¾à¦œ à¦•à¦°à§‡?

* MNIST dataset **download + load** à¦•à¦°à§‡
* Dataset already **train/test split à¦•à¦°à¦¾ à¦¥à¦¾à¦•à§‡**

## ðŸ‘‰ à¦•à§‡à¦¨ à¦¦à¦°à¦•à¦¾à¦°?

* Data à¦›à¦¾à§œà¦¾ model train à¦•à¦°à¦¾ à¦¯à¦¾à¦¬à§‡ à¦¨à¦¾
* Beginner-à¦¦à§‡à¦° à¦œà¦¨à§à¦¯ built-in dataset

---

## âœ… Syntax

```python
(x_train, y_train), (x_test, y_test) = mnist.load_data()
```

## ðŸ”¸ Parameters

```
à¦•à§‹à¦¨à§‹ parameter à¦²à¦¾à¦—à§‡ à¦¨à¦¾
```

## ðŸ” Returns (à¦–à§à¦¬ à¦—à§à¦°à§à¦¤à§à¦¬à¦ªà§‚à¦°à§à¦£)

* 2à¦Ÿà¦¾ tuple return à¦•à¦°à§‡
  1ï¸âƒ£ Training data
  2ï¸âƒ£ Testing data

---

## âœ… Code Example

```python
from tensorflow.keras.datasets import mnist

(x_train, y_train), (x_test, y_test) = mnist.load_data()

print(x_train.shape)  # (60000, 28, 28)
print(y_train.shape)  # (60000,)
```

ðŸ“Œ à¦à¦–à¦¾à¦¨à§‡:

* `x_train` â†’ image
* `y_train` â†’ label (0â€“9)

---

# ðŸ”¹ 2. `Sequential()`

## ðŸ‘‰ à¦•à§€ à¦•à¦¾à¦œ à¦•à¦°à§‡?

* Neural network model à¦¤à§ˆà¦°à¦¿ à¦•à¦°à§‡
* Layers à¦—à§à¦²à§‹à¦•à§‡ **à¦à¦•à¦Ÿà¦¾à¦° à¦ªà¦° à¦à¦•à¦Ÿà¦¾ stack à¦•à¦°à§‡**

## ðŸ‘‰ à¦•à¦–à¦¨ à¦¬à§à¦¯à¦¬à¦¹à¦¾à¦° à¦•à¦°à¦¬à§‡?

* Single input
* Single output
* No branching / skip connection

---

## âœ… Syntax

```python
model = Sequential(layers)
```

## ðŸ”¸ Parameter

| Name   | Meaning       |
| ------ | ------------- |
| layers | layer à¦à¦° list |

## ðŸ” Returns

```
model object
```

---

## âœ… Code Example

```python
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

model = Sequential([
    Dense(32, activation='relu', input_shape=(10,)),
    Dense(1)
])
```

ðŸ“Œ à¦à¦–à¦¨ `model` object à¦¦à¦¿à§Ÿà§‡ à¦¸à¦¬ à¦•à¦¾à¦œ à¦¹à¦¬à§‡

---

# ðŸ”¹ 3. `Dense()`

## ðŸ‘‰ à¦•à§€ à¦•à¦¾à¦œ à¦•à¦°à§‡?

* Fully Connected Layer
* Input feature à¦—à§à¦²à§‹ combine à¦•à¦°à§‡ output à¦¦à§‡à§Ÿ

---

## âœ… Syntax

```python
Dense(units, activation=None)
```

## ðŸ”¸ Parameters

| Parameter  | Mandatory | Example |
| ---------- | --------- | ------- |
| units      | âœ…         | 32      |
| activation | âŒ         | 'relu'  |

---

## ðŸ” Returns

```
Dense layer object
```

---

## âœ… à¦•à§‹à¦¨ à¦•à§à¦·à§‡à¦¤à§à¦°à§‡ à¦•à§‹à¦¨ Dense?

### ðŸ”¹ Hidden layer

```python
Dense(64, activation='relu')
```

### ðŸ”¹ Binary classification

```python
Dense(1, activation='sigmoid')
```

### ðŸ”¹ Multi-class classification

```python
Dense(10, activation='softmax')
```

---

# ðŸ”¹ 4. `Flatten()`

## ðŸ‘‰ à¦•à§€ à¦•à¦¾à¦œ à¦•à¦°à§‡?

* Image à¦¬à¦¾ multi-dimensional data â†’ 1D vector à¦¬à¦¾à¦¨à¦¾à§Ÿ

---

## âœ… Syntax

```python
Flatten()
```

## ðŸ”¸ Parameters

```
None
```

## ðŸ” Returns

```
Flatten layer object
```

---

## âœ… Code Example

```python
from tensorflow.keras.layers import Flatten

# 28x28 image â†’ 784 vector
Flatten(input_shape=(28,28))
```

ðŸ“Œ Dense layer image directly à¦¨à¦¿à¦¤à§‡ à¦ªà¦¾à¦°à§‡ à¦¨à¦¾, à¦¤à¦¾à¦‡ Flatten à¦¦à¦°à¦•à¦¾à¦°

---

# ðŸ”¹ 5. `model.compile()`

## ðŸ‘‰ à¦•à§€ à¦•à¦¾à¦œ à¦•à¦°à§‡?

* Model-à¦•à§‡ **training-ready** à¦•à¦°à§‡
* à¦¬à¦²à§‡ à¦¦à§‡à§Ÿ:

  * à¦•à§€à¦­à¦¾à¦¬à§‡ weight update à¦¹à¦¬à§‡
  * error à¦•à§€à¦­à¦¾à¦¬à§‡ calculate à¦¹à¦¬à§‡

---

## âœ… Syntax (Minimal)

```python
model.compile(optimizer, loss)
```

## ðŸ”¸ Mandatory Parameters

| Parameter | Meaning            |
| --------- | ------------------ |
| optimizer | weight update rule |
| loss      | error function     |

---

## âœ… Code Examples

### ðŸ”¹ MNIST (integer labels)

```python
model.compile(
    optimizer='adam',
    loss='sparse_categorical_crossentropy'
)
```

### ðŸ”¹ Binary classification

```python
model.compile(
    optimizer='adam',
    loss='binary_crossentropy'
)
```

---

## ðŸ” Returns

```
None
```

ðŸ“Œ à¦•à¦¿à¦¨à§à¦¤à§ internal configuration à¦¤à§ˆà¦°à¦¿ à¦¹à§Ÿ

---



---

# ðŸ“˜ `model.compile()` â€“ Full Detailed Documentation

---

## ðŸ”¹ 1. `model.compile()` à¦•à§€?

ðŸ‘‰ `compile()` à¦¹à¦²à§‹ **model training-à¦à¦° à¦†à¦—à§‡ à¦¬à¦¾à¦§à§à¦¯à¦¤à¦¾à¦®à§‚à¦²à¦• à¦§à¦¾à¦ª**
ðŸ‘‰ à¦à¦Ÿà¦¾ model-à¦•à§‡ à¦¬à¦²à§‡ à¦¦à§‡à§Ÿ:

1ï¸âƒ£ **à¦•à§€à¦­à¦¾à¦¬à§‡ weight update à¦¹à¦¬à§‡** â†’ optimizer
2ï¸âƒ£ **à¦­à§à¦² (error) à¦•à§€à¦­à¦¾à¦¬à§‡ à¦®à¦¾à¦ªà¦¾ à¦¹à¦¬à§‡** â†’ loss
3ï¸âƒ£ **performance à¦•à§€à¦­à¦¾à¦¬à§‡ à¦¦à§‡à¦–à¦¾à¦¨à§‹ à¦¹à¦¬à§‡** â†’ metrics (optional)

ðŸ“Œ à¦¸à¦¹à¦œ à¦­à¦¾à¦·à¦¾à§Ÿ:

> **`compile()` = training rules set à¦•à¦°à¦¾**

---

## ðŸ”¹ 2. Minimal Syntax (à¦¸à¦¬à¦šà§‡à§Ÿà§‡ à¦›à§‹à¦Ÿ form)

```python
model.compile(optimizer, loss)
```

ðŸ“Œ à¦à¦Ÿà§à¦•à§ à¦¦à¦¿à¦²à§‡à¦‡ model train à¦•à¦°à¦¾ à¦¯à¦¾à¦¬à§‡

---

## ðŸ”¹ 3. Mandatory Parameters â­

### âœ… (1) `optimizer`

#### ðŸ‘‰ à¦•à§€ à¦•à¦¾à¦œ à¦•à¦°à§‡?

* Loss à¦•à¦®à¦¾à¦¨à§‹à¦° à¦œà¦¨à§à¦¯ **weights à¦•à§€à¦­à¦¾à¦¬à§‡ change à¦¹à¦¬à§‡** à¦¸à§‡à¦Ÿà¦¾ à¦ à¦¿à¦• à¦•à¦°à§‡
* Gradient descent-à¦à¦° strategy

---

#### ðŸ”¸ Common Optimizers

| Optimizer | à¦•à¦–à¦¨ à¦¬à§à¦¯à¦¬à¦¹à¦¾à¦°                   |
| --------- | ----------------------------- |
| `sgd`     | Basic learning                |
| `adam` â­  | Most popular (default choice) |
| `rmsprop` | RNN / noisy data              |

---

#### âœ… Minimal Example

```python
optimizer='adam'
```

ðŸ“Œ Beginner + MNIST + most DL problem-à¦ **adam best**

---

### âœ… (2) `loss`

#### ðŸ‘‰ à¦•à§€ à¦•à¦¾à¦œ à¦•à¦°à§‡?

* Model prediction à¦†à¦° **true label-à¦à¦° à¦ªà¦¾à¦°à§à¦¥à¦•à§à¦¯** à¦®à¦¾à¦ªà§‡
* Backpropagation à¦à¦‡ loss à¦¦à¦¿à§Ÿà§‡à¦‡ à¦¹à§Ÿ

---

## ðŸ”¹ 4. Loss Function Selection (Very Important â­)

### ðŸ”¹ Case-wise Loss Table

| Problem Type                | Output Layer      | Loss Function                   |
| --------------------------- | ----------------- | ------------------------------- |
| Regression                  | Dense(1)          | mean_squared_error              |
| Binary classification       | Dense(1, sigmoid) | binary_crossentropy             |
| Multi-class (integer label) | Dense(n, softmax) | sparse_categorical_crossentropy |
| Multi-class (one-hot label) | Dense(n, softmax) | categorical_crossentropy        |

---

### âœ… MNIST (integer labels: 0â€“9)

```python
model.compile(
    optimizer='adam',
    loss='sparse_categorical_crossentropy'
)
```

ðŸ“Œ à¦•à¦¾à¦°à¦£:

* Labels â†’ integer (0,1,2â€¦9)
* One-hot encoding à¦•à¦°à¦¾ à¦¹à§Ÿà¦¨à¦¿

---

### âœ… Binary Classification

```python
model.compile(
    optimizer='adam',
    loss='binary_crossentropy'
)
```

ðŸ“Œ Output:

```python
Dense(1, activation='sigmoid')
```

---

### âœ… Regression Example

```python
model.compile(
    optimizer='adam',
    loss='mean_squared_error'
)
```

---

## ðŸ”¹ 5. Optional Parameter: `metrics` (Monitoring Only)

```python
model.compile(
    optimizer='adam',
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)
```

ðŸ“Œ à¦®à¦¨à§‡ à¦°à¦¾à¦–à¦¬à§‡:

* **metrics training change à¦•à¦°à§‡ à¦¨à¦¾**
* à¦¶à§à¦§à§ performance à¦¦à§‡à¦–à¦¾à§Ÿ

---

## ðŸ”¹ 6. `compile()` internally à¦•à§€ à¦•à¦°à§‡? (Concept)

`compile()` à¦•à¦°à¦¾à¦° à¦¸à¦®à§Ÿ model:

1ï¸âƒ£ Optimizer object à¦¤à§ˆà¦°à¦¿ à¦•à¦°à§‡
2ï¸âƒ£ Loss function attach à¦•à¦°à§‡
3ï¸âƒ£ Metrics tracker attach à¦•à¦°à§‡
4ï¸âƒ£ Training graph à¦ªà§à¦°à¦¸à§à¦¤à§à¦¤ à¦•à¦°à§‡

ðŸ“Œ à¦à¦‡ à¦§à¦¾à¦ª à¦›à¦¾à§œà¦¾ `fit()` à¦œà¦¾à¦¨à§‡ à¦¨à¦¾:

* à¦•à§€ optimize à¦•à¦°à¦¬à§‡
* à¦•à§€ minimize à¦•à¦°à¦¬à§‡

---

## ðŸ”¹ 7. `compile()` à¦•à§€ return à¦•à¦°à§‡?

```
None
```

â— à¦•à¦¿à¦¨à§à¦¤à§:

* Model object-à¦à¦° à¦­à¦¿à¦¤à¦°à§‡ **internal state à¦¤à§ˆà¦°à¦¿ à¦¹à§Ÿ**

---

## ðŸ”¹ 8. à¦•à§‡à¦¨ `compile()` à¦›à¦¾à§œà¦¾ `fit()` à¦•à¦¾à¦œ à¦•à¦°à§‡ à¦¨à¦¾?

âŒ Wrong

```python
model.fit(x_train, y_train, epochs=5)
```

ðŸ“Œ Error:

```
You must compile your model before training.
```

à¦•à¦¾à¦°à¦£:

* Optimizer à¦¨à§‡à¦‡
* Loss à¦¨à§‡à¦‡
* Training rule undefined

---

## ðŸ”¹ 9. Minimal End-to-End Example

```python
model = Sequential([
    Flatten(input_shape=(28,28)),
    Dense(32, activation='relu'),
    Dense(10, activation='softmax')
])

model.compile(
    optimizer='adam',
    loss='sparse_categorical_crossentropy'
)

model.fit(x_train, y_train, epochs=5)
```

---

## ðŸ”¹ 10. Common Mistakes âŒ (Exam Favorite)

### âŒ Wrong loss for integer labels

```python
loss='categorical_crossentropy'  # WRONG
```

âœ” Correct:

```python
loss='sparse_categorical_crossentropy'
```

---

### âŒ Regression-à¦ accuracy

```python
metrics=['accuracy']  # WRONG
```

---

### âŒ Binary output à¦•à¦¿à¦¨à§à¦¤à§ softmax loss

```python
Dense(1, activation='sigmoid')
loss='categorical_crossentropy'  # WRONG
```

---

## ðŸ”¹ 11. `compile()` vs `fit()` vs `evaluate()`

| Function | Role               |
| -------- | ------------------ |
| compile  | Training rules set |
| fit      | Model à¦¶à§‡à¦–à§‡         |
| evaluate | Model test à¦¹à§Ÿ      |

---

## ðŸ§  Exam / Viva One-Liners â­

* **`compile()` makes model training-ready**
* **optimizer controls weight update**
* **loss controls learning**
* **compile before fit is mandatory**

---

## âœ… Final Takeaway (Golden Line)

> ðŸ”¹ **`model.compile()` defines HOW the model will learn**
> ðŸ”¹ Without compile â†’ no training possible

---


# ðŸ”¹ 6. `model.fit()`

## ðŸ‘‰ à¦•à§€ à¦•à¦¾à¦œ à¦•à¦°à§‡?

* Model training à¦¶à§à¦°à§ à¦•à¦°à§‡
* Data à¦¦à§‡à¦–à§‡ weight à¦¶à¦¿à¦–à§‡

---

## âœ… Syntax (Minimal)

```python
model.fit(x, y, epochs)
```

## ðŸ”¸ Parameters

| Name   | Meaning             |
| ------ | ------------------- |
| x      | training data       |
| y      | training labels     |
| epochs | à¦•à¦¤à¦¬à¦¾à¦° dataset à¦¦à§‡à¦–à¦¬à§‡ |

---

## ðŸ” Returns

```
History object
```

---

## âœ… Code Example

```python
history = model.fit(x_train, y_train, epochs=5)

print(history.history.keys())
```

ðŸ“Œ loss, accuracy track à¦•à¦°à¦¾ à¦¯à¦¾à§Ÿ

---

---

# ðŸ“˜ `model.fit()` â€“ Full Detailed Documentation

---

## ðŸ”¹ 1. `model.fit()` à¦•à§€ à¦•à¦¾à¦œ à¦•à¦°à§‡?

ðŸ‘‰ Neural network-à¦•à§‡ **train** à¦•à¦°à§‡
ðŸ‘‰ Data à¦¦à§‡à¦–à¦¿à§Ÿà§‡ **weights update** à¦•à¦°à§‡
ðŸ‘‰ Loss à¦•à¦®à¦¾à¦¨à§‹à¦° à¦šà§‡à¦·à§à¦Ÿà¦¾ à¦•à¦°à§‡

ðŸ“Œ à¦¸à¦¹à¦œ à¦•à¦¥à¦¾à§Ÿ:

> **`fit()` = model à¦¶à§‡à¦–à§‡**

---

## ðŸ”¹ 2. Minimal Syntax (à¦¸à¦¬à¦šà§‡à§Ÿà§‡ à¦›à§‹à¦Ÿ form)

```python
history = model.fit(x, y, epochs)
```

à¦à¦Ÿà¦¾à¦‡ **à¦¸à¦¬à¦šà§‡à§Ÿà§‡ minimum working call**à¥¤

---

## ðŸ”¹ 3. Minimal Required Parameters â­

### âœ… `x` â€” Training Data

| à¦¬à¦¿à¦·à§Ÿ    | Explanation                  |
| ------- | ---------------------------- |
| Type    | numpy array / tensor         |
| Meaning | Input features               |
| Shape   | `(num_samples, input_shape)` |

#### Example (MNIST)

```python
x_train.shape = (60000, 28, 28)
```

---

### âœ… `y` â€” Training Labels

| à¦¬à¦¿à¦·à§Ÿ    | Explanation                                      |
| ------- | ------------------------------------------------ |
| Type    | numpy array / tensor                             |
| Meaning | True output                                      |
| Shape   | `(num_samples,)` à¦¬à¦¾ `(num_samples, num_classes)` |

#### Example

```python
y_train.shape = (60000,)
```

ðŸ“Œ `x` à¦†à¦° `y`-à¦° **first dimension à¦à¦•à¦‡ à¦¹à¦¤à§‡ à¦¹à¦¬à§‡**

---

### âœ… `epochs` â€” Training Loop Count

| à¦¬à¦¿à¦·à§Ÿ    | Explanation                    |
| ------- | ------------------------------ |
| Type    | int                            |
| Meaning | Dataset à¦•à§Ÿà¦¬à¦¾à¦° à¦ªà§à¦°à§‹à¦Ÿà¦¾ train à¦¹à¦¬à§‡ |
| Example | `epochs=5`                     |

ðŸ“Œ `epochs=1` à¦®à¦¾à¦¨à§‡:

> à¦ªà§à¦°à§‹ training data à¦à¦•à¦¬à¦¾à¦° à¦¦à§‡à¦–à¦¾

---

## ðŸ”¹ 4. Parameter à¦¨à¦¾ à¦¦à¦¿à¦²à§‡ à¦•à§€ à¦¹à¦¬à§‡?

âŒ Wrong

```python
model.fit(x_train, y_train)
```

ðŸ“Œ Error à¦¹à¦¬à§‡, à¦•à¦¾à¦°à¦£:

* `epochs` mandatory

---

## ðŸ”¹ 5. `model.fit()` internally à¦•à§€ à¦•à¦°à§‡? (Step-by-Step)

à¦ªà§à¦°à¦¤à¦¿à¦Ÿà¦¾ epoch-à¦ ðŸ‘‡

1ï¸âƒ£ `x` à¦¨à§‡à§Ÿ
2ï¸âƒ£ Forward pass à¦•à¦°à§‡
3ï¸âƒ£ Prediction à¦¬à§‡à¦° à¦•à¦°à§‡
4ï¸âƒ£ Loss calculate à¦•à¦°à§‡
5ï¸âƒ£ Backpropagation
6ï¸âƒ£ Weight update
7ï¸âƒ£ Metric calculate à¦•à¦°à§‡

ðŸ“Œ à¦à¦‡ cycle **epochs à¦¬à¦¾à¦° repeat à¦¹à§Ÿ**

---

## ðŸ”¹ 6. `model.fit()` à¦•à§€ return à¦•à¦°à§‡? â­

### ðŸ” Return Type

```
History object
```

---

## ðŸ”¹ 7. `History object` à¦•à§€?

ðŸ‘‰ Training-à¦à¦° à¦¸à¦®à§Ÿ **à¦¸à¦¬ metric record à¦•à¦°à§‡**
ðŸ‘‰ Python object à¦†à¦•à¦¾à¦°à§‡ à¦¥à¦¾à¦•à§‡

---

### ðŸ” Structure

```python
history.history
```

à¦à¦Ÿà¦¾ à¦à¦•à¦Ÿà¦¾ dictionary ðŸ‘‡

```python
{
  'loss': [...],
  'accuracy': [...]
}
```

---

### âœ… Code Example

```python
history = model.fit(x_train, y_train, epochs=5)

print(history.history.keys())
```

Output:

```
dict_keys(['loss', 'accuracy'])
```

---

### ðŸ”¹ Epoch-wise value access

```python
print(history.history['loss'])
print(history.history['accuracy'])
```

ðŸ“Œ Plot à¦•à¦°à¦¾à¦° à¦œà¦¨à§à¦¯ à¦¬à§à¦¯à¦¬à¦¹à¦¾à¦° à¦¹à§Ÿ

---

## ðŸ”¹ 8. Metrics à¦¥à¦¾à¦•à¦²à§‡ à¦•à§€ à¦¹à§Ÿ?

### Compile:

```python
model.compile(
    optimizer='adam',
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)
```

### Fit output:

```
Epoch 1/5
loss: 0.42 - accuracy: 0.88
```

ðŸ“Œ `metrics` à¦¨à¦¾ à¦¦à¦¿à¦²à§‡ à¦¶à§à¦§à§ loss à¦¦à§‡à¦–à¦¾à¦¬à§‡

---

## ðŸ”¹ 9. Common Optional Parameters (Understanding Purpose)

(Exam-à¦ à¦¨à¦¾ à¦à¦²à§‡à¦“ concept à¦œà¦¾à¦¨à¦¾ à¦¦à¦°à¦•à¦¾à¦°)

| Parameter       | Purpose                |
| --------------- | ---------------------- |
| batch_size      | à¦à¦•à¦¸à¦¾à¦¥à§‡ à¦•à§Ÿà¦Ÿà¦¾ sample     |
| validation_data | validation performance |
| verbose         | output style           |

ðŸ“Œ à¦•à¦¿à¦¨à§à¦¤à§ **minimum training-à¦à¦° à¦œà¦¨à§à¦¯ à¦à¦—à§à¦²à§‹ à¦¦à¦°à¦•à¦¾à¦° à¦¨à§‡à¦‡**

---

## ðŸ”¹ 10. Very Common Beginner Mistakes âŒ

### âŒ x, y shape mismatch

```python
x.shape = (1000, 28, 28)
y.shape = (900,)
```

---

### âŒ Fit before compile

```python
model.fit(...)   # ERROR
```

ðŸ“Œ `compile()` mandatory

---

### âŒ Expecting prediction from fit

```python
y_pred = model.fit(...)  # WRONG
```

ðŸ“Œ Prediction â†’ `predict()`

---

## ðŸ”¹ 11. `fit()` vs `evaluate()` vs `predict()`

| Function | Learns | Needs y | Returns       |
| -------- | ------ | ------- | ------------- |
| fit      | âœ…      | âœ…       | History       |
| evaluate | âŒ      | âœ…       | loss, metrics |
| predict  | âŒ      | âŒ       | predictions   |

---

## ðŸ”¹ 12. Minimal End-to-End Example

```python
model.compile(
    optimizer='adam',
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

history = model.fit(x_train, y_train, epochs=5)
```

---

## ðŸ§  Exam / Viva One-Liners â­

* **`model.fit()` trains the model**
* **epochs defines training repetitions**
* **returns History object**
* **compile() before fit() is mandatory**

---

## âœ… Final Takeaway

* `model.fit()`-à¦à¦° **minimum parameters = x, y, epochs**
* Training-à¦à¦° à¦¸à¦¬ à¦¤à¦¥à§à¦¯ **History object-à¦ à¦¥à¦¾à¦•à§‡**
* Learning only happens here

---


# ðŸ”¹ 7. `model.evaluate()`

## ðŸ‘‰ à¦•à§€ à¦•à¦¾à¦œ à¦•à¦°à§‡?

* Trained model à¦•à¦¤à¦Ÿà¦¾ à¦­à¦¾à¦²à§‹ à¦•à¦¾à¦œ à¦•à¦°à¦›à§‡ à¦¸à§‡à¦Ÿà¦¾ à¦®à¦¾à¦ªà§‡

---

## âœ… Syntax

```python
model.evaluate(x, y)
```

## ðŸ”¸ Parameters

| Name | Meaning     |
| ---- | ----------- |
| x    | test data   |
| y    | true labels |

---

## ðŸ” Returns

```
loss, metrics
```

---

## âœ… Code Example

```python
loss, acc = model.evaluate(x_test, y_test)
print("Accuracy:", acc)
```


---

# ðŸ“˜ `model.evaluate()` â€“ Full Detailed Explanation (Minimal Focus)

---

## ðŸ”¹ 1. `model.evaluate()` à¦•à§€ à¦•à¦¾à¦œ à¦•à¦°à§‡?

ðŸ‘‰ Training à¦¶à§‡à¦· à¦¹à¦“à§Ÿà¦¾à¦° à¦ªà¦°
ðŸ‘‰ Model à¦¶à§‡à¦–à¦¾ weight à¦¬à§à¦¯à¦¬à¦¹à¦¾à¦° à¦•à¦°à§‡
ðŸ‘‰ **Test / validation data-à¦¤à§‡ model-à¦à¦° performance à¦®à¦¾à¦ªà§‡**

ðŸ“Œ à¦à¦Ÿà¦¾ **training à¦•à¦°à§‡ à¦¨à¦¾**
ðŸ“Œ à¦à¦Ÿà¦¾ **prediction à¦¦à§‡à§Ÿ à¦¨à¦¾**
ðŸ“Œ à¦¶à§à¦§à§ **loss + metric calculate à¦•à¦°à§‡**

---

## ðŸ”¹ 2. Minimal Syntax (à¦¸à¦¬à¦šà§‡à§Ÿà§‡ à¦›à§‹à¦Ÿ form)

```python
result = model.evaluate(x, y)
```

à¦¬à¦¾ (à¦¸à¦¬à¦šà§‡à§Ÿà§‡ common)

```python
loss, metric = model.evaluate(x, y)
```

---

## ðŸ”¹ 3. Minimal Required Parameters â­

### âœ… `x` (MANDATORY)

| à¦¬à¦¿à¦·à§Ÿ           | Explanation                        |
| -------------- | ---------------------------------- |
| Parameter name | `x`                                |
| Type           | numpy array / tensor               |
| Meaning        | Test / validation input data       |
| Shape          | `(number_of_samples, input_shape)` |

ðŸ“Œ `x` = input data
ðŸ“Œ Training data à¦¨à§Ÿ, à¦¸à¦¾à¦§à¦¾à¦°à¦£à¦¤ **test data**

---

### âœ… `y` (MANDATORY)

| à¦¬à¦¿à¦·à§Ÿ           | Explanation                                                  |
| -------------- | ------------------------------------------------------------ |
| Parameter name | `y`                                                          |
| Type           | numpy array / tensor                                         |
| Meaning        | True labels (ground truth)                                   |
| Shape          | `(number_of_samples,)` à¦¬à¦¾ `(number_of_samples, num_classes)` |

ðŸ“Œ `y` à¦›à¦¾à§œà¦¾ loss calculate à¦•à¦°à¦¾ à¦¸à¦®à§à¦­à¦¬ à¦¨à¦¾

---

## ðŸ”¹ 4. Parameter à¦¨à¦¾ à¦¦à¦¿à¦²à§‡ à¦•à§€ à¦¹à¦¬à§‡?

âŒ Wrong

```python
model.evaluate(x_test)
```

ðŸ“Œ Error à¦†à¦¸à¦¬à§‡, à¦•à¦¾à¦°à¦£:

* loss function à¦•à§‡ true label à¦¦à¦°à¦•à¦¾à¦°

---

## ðŸ”¹ 5. `model.evaluate()` à¦•à§€ return à¦•à¦°à§‡? â­

### ðŸ” Return Type

```
float à¦…à¦¥à¦¬à¦¾ list of floats
```

Return format **depend à¦•à¦°à§‡** `compile()`-à¦ à¦•à§€ à¦¦à¦¿à§Ÿà§‡à¦› à¦¤à¦¾à¦° à¦‰à¦ªà¦°à¥¤

---

## ðŸ”¹ 6. Compile à¦…à¦¨à§à¦¯à¦¾à§Ÿà§€ Return Value (Very Important)

---

### ðŸ”¹ Case 1: Only loss à¦¦à§‡à¦“à§Ÿà¦¾ à¦†à¦›à§‡

```python
model.compile(
    optimizer='adam',
    loss='sparse_categorical_crossentropy'
)
```

### ðŸ” Return

```python
loss = model.evaluate(x_test, y_test)
```

ðŸ“Œ Single float return à¦•à¦°à§‡
ðŸ“Œ Example:

```
0.2456
```

---

### ðŸ”¹ Case 2: Loss + One Metric

```python
model.compile(
    optimizer='adam',
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)
```

### ðŸ” Return

```python
loss, accuracy = model.evaluate(x_test, y_test)
```

ðŸ“Œ Two values return à¦•à¦°à§‡
ðŸ“Œ Example:

```
loss = 0.24
accuracy = 0.92
```

---

### ðŸ”¹ Case 3: Multiple Metrics

```python
model.compile(
    optimizer='adam',
    loss='binary_crossentropy',
    metrics=['accuracy', 'precision']
)
```

### ðŸ” Return

```python
loss, acc, prec = model.evaluate(x_test, y_test)
```

ðŸ“Œ Return order:

```
[loss, metric1, metric2, ...]
```

---

## ðŸ”¹ 7. Return Value-à¦à¦° Shape / Meaning

| Return value       | Meaning                       |
| ------------------ | ----------------------------- |
| loss               | Model-à¦à¦° average error        |
| accuracy           | Correct prediction percentage |
| precision / recall | Advanced metric               |

ðŸ“Œ à¦¸à¦¬ value **test dataset-à¦à¦° à¦‰à¦ªà¦° average**

---

## ðŸ”¹ 8. Full Minimal Code Example (MNIST)

```python
loss, acc = model.evaluate(x_test, y_test)

print("Test Loss:", loss)
print("Test Accuracy:", acc)
```

---

## ðŸ”¹ 9. Difference: `evaluate()` vs `predict()`

| Feature            | evaluate() | predict() |
| ------------------ | ---------- | --------- |
| Needs true label   | âœ…          | âŒ         |
| Returns loss       | âœ…          | âŒ         |
| Returns prediction | âŒ          | âœ…         |
| Training           | âŒ          | âŒ         |

---

## ðŸ”¹ 10. Very Common Beginner Confusions âŒ

### âŒ Using training data in evaluate

```python
model.evaluate(x_train, y_train)  # WRONG practice
```

ðŸ“Œ Test data à¦¬à§à¦¯à¦¬à¦¹à¦¾à¦° à¦•à¦°à¦¾ à¦‰à¦šà¦¿à¦¤

---

### âŒ Expecting class labels

```python
loss, y_pred = model.evaluate(x_test, y_test)  # WRONG
```

ðŸ“Œ `evaluate()` prediction à¦¦à§‡à§Ÿ à¦¨à¦¾

---

## ðŸ”¹ 11. Exam / Viva One-Liners â­

* **`model.evaluate()` needs both input data and true labels**
* **It returns loss and metrics, not predictions**
* **It is used for testing, not training**

---

## âœ… Final Takeaway

* `model.evaluate()`-à¦à¦° **minimum parameters = x, y**
* Return value à¦¨à¦¿à¦°à§à¦­à¦° à¦•à¦°à§‡ **compile() à¦ à¦¦à§‡à¦“à§Ÿà¦¾ metrics-à¦à¦° à¦‰à¦ªà¦°**
* Model performance measure à¦•à¦°à¦¾à¦° à¦à¦•à¦®à¦¾à¦¤à§à¦° standard method

---


---

# ðŸ”¹ 8. `model.predict()`

## ðŸ‘‰ à¦•à§€ à¦•à¦¾à¦œ à¦•à¦°à§‡?

* New data à¦¦à¦¿à§Ÿà§‡ prediction à¦¦à§‡à§Ÿ

---

## âœ… Syntax

```python
model.predict(x)
```

## ðŸ” Returns

* Regression â†’ value
* Binary â†’ probability
* Multi-class â†’ probability vector

---

## âœ… Code Example

```python
import numpy as np

pred = model.predict(x_test)

print(pred[0])              # probabilities
print(np.argmax(pred[0]))   # predicted class
```




---

# ðŸ“˜ `model.predict()` â€“ Full Detailed Explanation (Minimal Focus)

---

## ðŸ”¹ 1. `model.predict()` à¦•à§€ à¦•à¦¾à¦œ à¦•à¦°à§‡?

ðŸ‘‰ Training à¦¶à§‡à¦· à¦¹à¦“à§Ÿà¦¾à¦° à¦ªà¦°
ðŸ‘‰ Model à¦¶à§‡à¦–à¦¾ weight à¦¬à§à¦¯à¦¬à¦¹à¦¾à¦° à¦•à¦°à§‡
ðŸ‘‰ **à¦¨à¦¤à§à¦¨ / à¦…à¦œà¦¾à¦¨à¦¾ data-à¦à¦° output à¦…à¦¨à§à¦®à¦¾à¦¨ (predict)** à¦•à¦°à§‡

ðŸ“Œ `predict()` **training à¦•à¦°à§‡ à¦¨à¦¾**, à¦¶à§à¦§à§ inference à¦•à¦°à§‡à¥¤

---

## ðŸ”¹ 2. Minimal Syntax (à¦¸à¦¬à¦šà§‡à§Ÿà§‡ à¦›à§‹à¦Ÿ form)

```python
pred = model.predict(x)
```

---

## ðŸ”¹ 3. Minimal Required Parameter â­

### âœ… `x` (MANDATORY)

| à¦¬à¦¿à¦·à§Ÿ           | Explanation                        |
| -------------- | ---------------------------------- |
| Parameter name | `x`                                |
| Type           | numpy array / tensor               |
| Meaning        | Input data                         |
| Shape          | `(number_of_samples, input_shape)` |

ðŸ“Œ **`x` à¦›à¦¾à§œà¦¾ `predict()` à¦•à¦¾à¦œ à¦•à¦°à¦¬à§‡ à¦¨à¦¾**

---

### ðŸ” `x` à¦•à§€ value à¦¨à¦¿à¦¤à§‡ à¦ªà¦¾à¦°à§‡?

| Model input   | `x` shape          |
| ------------- | ------------------ |
| Tabular       | `(N, features)`    |
| MNIST image   | `(N, 28, 28)`      |
| CNN image     | `(N, H, W, C)`     |
| Single sample | `(1, input_shape)` |

---

## ðŸ”¹ 4. `predict()` à¦•à§€ return à¦•à¦°à§‡? â­ (Very Important)

### ðŸ” Return Type

```
numpy.ndarray
```

---

## ðŸ”¹ 5. Task-wise Return Value (Detail)

---

### ðŸ”¹ Case 1: Regression Model

```python
Dense(1, activation='linear')
```

### ðŸ” Return

```
shape = (N, 1)
```

Example:

```python
[[23.5],
 [18.2]]
```

ðŸ“Œ Predicted continuous value

---

### ðŸ”¹ Case 2: Binary Classification

```python
Dense(1, activation='sigmoid')
```

### ðŸ” Return

```
shape = (N, 1)
```

Example:

```python
[[0.87],
 [0.12]]
```

ðŸ“Œ Probability
ðŸ“Œ Class rule:

```python
prob > 0.5 â†’ class 1
```

---

### ðŸ”¹ Case 3: Multi-Class Classification (MNIST)

```python
Dense(10, activation='softmax')
```

### ðŸ” Return

```
shape = (N, 10)
```

Example:

```python
[0.01, 0.02, 0.90, 0.01, ...]
```

ðŸ“Œ Each value = class probability
ðŸ“Œ Sum = 1

---

## ðŸ”¹ 6. Single Sample Prediction (Very Common Confusion)

### âŒ Wrong

```python
model.predict(x_test[0])   # shape mismatch
```

### âœ… Correct

```python
model.predict(x_test[0:1])
```

ðŸ“Œ Reason:

* Model expects **batch dimension**
* Shape must be `(1, 28, 28)`

---

## ðŸ”¹ 7. Converting Prediction â†’ Class Label

### Multi-class

```python
pred = model.predict(x)
predicted_class = np.argmax(pred, axis=1)
```

---

### Binary

```python
pred = model.predict(x)
predicted_class = (pred > 0.5).astype(int)
```

---

## ðŸ”¹ 8. Full Minimal Example (MNIST)

```python
pred = model.predict(x_test)

print(pred.shape)        # (10000, 10)
print(pred[0])           # probabilities
print(np.argmax(pred[0]))  # predicted digit
```

---

## ðŸ”¹ 9. Summary Table (Exam Gold â­)

| Aspect            | Detail               |
| ----------------- | -------------------- |
| Function          | `model.predict()`    |
| Minimal parameter | `x`                  |
| Parameter type    | numpy array / tensor |
| Returns           | numpy array          |
| Regression        | value                |
| Binary            | probability          |
| Multi-class       | probability vector   |

---

## ðŸ”¹ 10. Viva / Exam One-Liner

> **`model.predict()` takes only input data and returns model output without training.**

---

## âœ… Final Takeaway

* `predict()`-à¦à¦° **only mandatory parameter = input data**
* Output à¦¸à¦¬à¦¸à¦®à§Ÿ **array à¦†à¦•à¦¾à¦°à§‡ à¦†à¦¸à§‡**
* Class à¦¬à§‡à¦° à¦•à¦°à¦¤à§‡ **post-processing** (argmax / threshold) à¦²à¦¾à¦—à§‡
* Training à¦†à¦° prediction à¦†à¦²à¦¾à¦¦à¦¾ à¦§à¦¾à¦ª

---







---

# ðŸ”¹ 9. `model.summary()`

## ðŸ‘‰ à¦•à§€ à¦•à¦¾à¦œ à¦•à¦°à§‡?

* Model architecture table à¦†à¦•à¦¾à¦°à§‡ à¦¦à§‡à¦–à¦¾à§Ÿ

---

## âœ… Syntax

```python
model.summary()
```

## ðŸ” Returns

```
None (prints output)
```

---

## ðŸ§  Full Minimal Pipeline Example (All Together)

```python
from tensorflow.keras.datasets import mnist
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten

(x_train, y_train), (x_test, y_test) = mnist.load_data()

x_train = x_train / 255.0
x_test  = x_test / 255.0

model = Sequential([
    Flatten(input_shape=(28,28)),
    Dense(32, activation='relu'),
    Dense(10, activation='softmax')
])

model.compile(
    optimizer='adam',
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

model.fit(x_train, y_train, epochs=5)

model.evaluate(x_test, y_test)
model.predict(x_test)
```

---

## âœ… Final Big Picture (Exam Line)

> **Deep Learning workflow:**
> Data â†’ Model â†’ Compile â†’ Fit â†’ Evaluate â†’ Predict

---


