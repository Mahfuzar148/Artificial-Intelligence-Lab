
---

# ğŸ§¾ Data Scaling & Data Splitting â€” Full Documentation

à¦¤à§‹à¦®à¦¾à¦° à¦•à§‹à¦¡ (reference):

```python
# scale input and output  
max_x = x.max()
max_y = y.max()

x_scaled = x / max_x 
y_scaled = y / max_y

x_scaled = x_scaled.reshape(-1,1)
y_scaled = y_scaled.reshape(-1,1)

# ----- Split data (70% train ,10% val , 20% test )

x_train_val, x_test, y_train_val, y_test = train_test_split(
    x_scaled, y_scaled, test_size=0.2, random_state=42 
) 

x_train, x_val, y_train, y_val = train_test_split(
    x_train_val, y_train_val, test_size=0.125, random_state=42
)
```

---

# ğŸ”· PART 1: Data Scaling (Feature Scaling)

## ğŸ”¹ Data Scaling à¦•à§€?

ğŸ‘‰ **Data scaling** à¦®à¦¾à¦¨à§‡ à¦¹à¦²à§‹:

> à¦¡à§‡à¦Ÿà¦¾à¦•à§‡ à¦à¦•à¦Ÿà¦¾ à¦›à§‹à¦Ÿ, à¦¨à¦¿à¦°à§à¦¦à¦¿à¦·à§à¦Ÿ range-à¦à¦° à¦®à¦§à§à¦¯à§‡ à¦†à¦¨à¦¾
> à¦¯à¦¾à¦¤à§‡ model à¦¸à¦¹à¦œà§‡ à¦¶à¦¿à¦–à¦¤à§‡ à¦ªà¦¾à¦°à§‡

---

## ğŸ”¹ à¦•à§‡à¦¨ scaling à¦¦à¦°à¦•à¦¾à¦°?

### Neural Network / ML model-à¦ à¦¸à¦®à¦¸à§à¦¯à¦¾ à¦¹à§Ÿ à¦¯à¦¦à¦¿ scaling à¦¨à¦¾ à¦•à¦°à¦¾ à¦¹à§Ÿ:

* Gradient slow à¦¹à§Ÿ
* Training unstable à¦¹à§Ÿ
* Loss à¦¬à§‡à¦¶à¦¿ fluctuation à¦•à¦°à§‡
* Large value dominate à¦•à¦°à§‡

ğŸ“Œ à¦¤à¦¾à¦‡ **almost à¦¸à¦¬ ML/DL model-à¦ scaling à¦¦à¦°à¦•à¦¾à¦°**

---

## ğŸ”¹ à¦¤à§‹à¦®à¦¾à¦° Scaling Method: Max Scaling

```python
x_scaled = x / max_x
y_scaled = y / max_y
```

### à¦à¦Ÿà¦¾à¦•à§‡ à¦¬à¦²à§‡:

> **Max Scaling** à¦¬à¦¾ **Normalization (0â€“1)**

---

## ğŸ”¹ à¦•à§€ à¦¹à¦šà§à¦›à§‡ à¦à¦–à¦¾à¦¨à§‡?

à¦§à¦°à¦¾ à¦¯à¦¾à¦•:

```python
x = [0, 50, 100]
max_x = 100
```

à¦¤à¦¾à¦¹à¦²à§‡:

```python
x_scaled = [0/100, 50/100, 100/100]
         = [0.0, 0.5, 1.0]
```

âœ” à¦¸à¦¬ value à¦à¦–à¦¨ 0â€“1 à¦à¦° à¦®à¦§à§à¦¯à§‡
âœ” Training stable

---

## ğŸ”¹ à¦•à§‡à¦¨ input à¦à¦¬à¦‚ output à¦¦à§à¦Ÿà§‹à¦‡ scale à¦•à¦°à¦¾ à¦¹à§Ÿà§‡à¦›à§‡?

```python
x_scaled = x / max_x
y_scaled = y / max_y
```

âœ” Input scale â†’ model fast à¦¶à§‡à¦–à§‡
âœ” Output scale â†’ loss stable à¦¥à¦¾à¦•à§‡

ğŸ“Œ Regression problem-à¦ output scaling à¦–à§à¦¬ à¦—à§à¦°à§à¦¤à§à¦¬à¦ªà§‚à¦°à§à¦£

---

## ğŸ”¹ reshape à¦•à§‡à¦¨ à¦•à¦°à¦¾ à¦¹à§Ÿà§‡à¦›à§‡?

```python
x_scaled = x_scaled.reshape(-1,1)
```

### à¦•à¦¾à¦°à¦£:

Keras / scikit-learn expect à¦•à¦°à§‡:

```
(samples, features)
```

* `-1` â†’ NumPy à¦¨à¦¿à¦œà§‡ sample à¦¸à¦‚à¦–à§à¦¯à¦¾ à¦ à¦¿à¦• à¦•à¦°à§‡
* `1` â†’ 1 feature

âŒ reshape à¦¨à¦¾ à¦•à¦°à¦²à§‡:

```
ValueError / shape mismatch
```

---

## ğŸ”¹ Scaling à¦¨à¦¾ à¦•à¦°à¦²à§‡ à¦•à§€ à¦¹à¦¤à§‹?

| Without scaling   | With scaling       |
| ----------------- | ------------------ |
| Training slow     | Training fast      |
| Gradient unstable | Stable gradient    |
| Poor convergence  | Smooth convergence |

---

# ğŸ”· PART 2: Data Splitting (Train / Validation / Test)

---

## ğŸ”¹ Data Split à¦•à§‡à¦¨ à¦¦à¦°à¦•à¦¾à¦°?

ğŸ‘‰ Model-à¦•à§‡ à§©à¦Ÿà¦¾ à¦†à¦²à¦¾à¦¦à¦¾ à¦œà¦¿à¦¨à¦¿à¦¸ à¦¶à§‡à¦–à¦¾à¦¤à§‡ à¦¹à§Ÿ:

1ï¸âƒ£ Train â†’ à¦¶à§‡à¦–à¦¾
2ï¸âƒ£ Validation â†’ tune à¦•à¦°à¦¾
3ï¸âƒ£ Test â†’ final à¦ªà¦°à§€à¦•à§à¦·à¦¾

ğŸ“Œ **Test data à¦•à¦–à¦¨à§‹ training-à¦ à¦¬à§à¦¯à¦¬à¦¹à¦¾à¦° à¦•à¦°à¦¾ à¦¯à¦¾à¦¬à§‡ à¦¨à¦¾**

---

# ğŸ”¹ `train_test_split()` â€” Full Documentation

Import:

```python
from sklearn.model_selection import train_test_split
```

---

## ğŸ”¹ Full Syntax

```python
train_test_split(
    *arrays,
    test_size=None,
    train_size=None,
    random_state=None,
    shuffle=True,
    stratify=None
)
```

---

# ğŸ”¹ Mandatory Parameters (à¦…à¦¬à¦¶à§à¦¯à¦‡ à¦²à¦¾à¦—à¦¬à§‡)

### âœ… `*arrays`

```python
train_test_split(x, y)
```

* Split à¦•à¦°à¦¤à§‡ à¦šà¦¾à¦“ à¦à¦®à¦¨ array à¦—à§à¦²à§‹
* à¦à¦•à¦¸à¦¾à¦¥à§‡ pass à¦•à¦°à¦²à§‡ alignment à¦ à¦¿à¦• à¦¥à¦¾à¦•à§‡

âŒ à¦à¦•à¦Ÿà¦¾à¦‡ à¦¦à¦¿à¦²à§‡ target mismatch à¦¹à¦¤à§‡ à¦ªà¦¾à¦°à§‡

---

### âœ… `test_size` à¦…à¦¥à¦¬à¦¾ `train_size` (à¦à¦•à¦Ÿà¦¾ à¦¦à¦¿à¦²à§‡à¦‡ à¦šà¦²à¦¬à§‡)

```python
test_size=0.2
```

| à¦®à¦¾à¦¨   | à¦…à¦°à§à¦¥            |
| ----- | --------------- |
| `0.2` | 20% test        |
| `0.3` | 30% test        |
| `100` | 100 sample test |

âŒ à¦¦à§à¦Ÿà§‹à¦‡ à¦¨à¦¾ à¦¦à¦¿à¦²à§‡ â†’ error

---

# ğŸ”¹ Optional Parameters (à¦•à¦¿à¦¨à§à¦¤à§ à¦–à§à¦¬ à¦—à§à¦°à§à¦¤à§à¦¬à¦ªà§‚à¦°à§à¦£)

### `random_state`

```python
random_state=42
```

ğŸ‘‰ Seed value

* Same split à¦¬à¦¾à¦°à¦¬à¦¾à¦° à¦ªà§‡à¦¤à§‡
* Reproducibility

à¦¨à¦¾ à¦¦à¦¿à¦²à§‡ â†’ à¦ªà§à¦°à¦¤à¦¿à¦¬à¦¾à¦° à¦†à¦²à¦¾à¦¦à¦¾ split

---

### `shuffle`

```python
shuffle=True
```

* Data shuffle à¦¹à¦¬à§‡ à¦•à¦¿à¦¨à¦¾
* Default: `True`

âŒ Time-series data à¦¹à¦²à§‡ `False`

---

### `stratify`

```python
stratify=y
```

* Classification à¦ class ratio à¦¬à¦œà¦¾à§Ÿ à¦°à¦¾à¦–à§‡
* Regression à¦ à¦¸à¦¾à¦§à¦¾à¦°à¦£à¦¤ à¦²à¦¾à¦—à§‡ à¦¨à¦¾

---

# ğŸ”· à¦¤à§‹à¦®à¦¾à¦° Split Logic (Detailed Breakdown)

---

## Step 1ï¸âƒ£: Train+Val vs Test (80% / 20%)

```python
x_train_val, x_test, y_train_val, y_test = train_test_split(
    x_scaled, y_scaled, test_size=0.2, random_state=42
)
```

âœ” 20% â†’ test
âœ” 80% â†’ train+val

---

## Step 2ï¸âƒ£: Train vs Validation (70% / 10%)

```python
x_train, x_val, y_train, y_val = train_test_split(
    x_train_val, y_train_val, test_size=0.125, random_state=42
)
```

### à¦•à§‡à¦¨ `0.125`?

à¦•à¦¾à¦°à¦£:

```
80% Ã— 0.125 = 10%
```

âœ” Final split:

* Train = 70%
* Validation = 10%
* Test = 20%

---

## ğŸ”¹ Print output explanation

```python
print(len(x_train))  # ~70%
print(len(x_val))    # ~10%
print(len(x_test))   # ~20%
```

ğŸ“Œ Correct ML pipeline

---

# ğŸ”¥ Common Mistakes (VERY IMPORTANT)

âŒ Scaling à¦•à¦°à¦¾à¦° à¦†à¦—à§‡ split à¦¨à¦¾ à¦•à¦°à¦¾
âŒ Test data à¦¥à§‡à¦•à§‡ `max_x` à¦¬à§‡à¦° à¦•à¦°à¦¾ (data leakage)
âŒ `random_state` à¦¨à¦¾ à¦¦à§‡à¦“à§Ÿà¦¾
âŒ Validation data à¦¦à¦¿à§Ÿà§‡ model train à¦•à¦°à¦¾

---

# ğŸ§  Best Practice (Industry Standard)

```text
1. Split data
2. Fit scaler on train only
3. Transform val & test
```

(à¦¤à§‹à¦®à¦¾à¦° à¦‰à¦¦à¦¾à¦¹à¦°à¦£ simple demo, à¦¤à¦¾à¦‡ acceptable)

---

# ğŸ§ª Alternative Scaling Methods (Reference)

| Method          | Use           |
| --------------- | ------------- |
| Min-Max Scaling | 0â€“1 range     |
| StandardScaler  | mean=0, std=1 |
| RobustScaler    | outlier safe  |

---

# ğŸ“Œ Summary Table (Exam Ready)

| Topic          | Explanation      |
| -------------- | ---------------- |
| Scaling        | Normalize values |
| Why scale      | Stable training  |
| `test_size`    | Test ratio       |
| `random_state` | Reproducibility  |
| `shuffle`      | Data mixing      |
| `stratify`     | Class balance    |

---

## ğŸ§  One-line Interview Answer

> Data scaling normalizes feature ranges for stable training, and `train_test_split` separates data into train, validation, and test sets to fairly evaluate model performance.

---


---

# âœ… FULL WORKING CODE

## (Data Scaling + Train/Val/Test Split with Explanation)

```python
# =========================
# Imports
# =========================
import numpy as np
from sklearn.model_selection import train_test_split


# =========================
# Example polynomial function
# =========================
def my_polynomial(x):
    # y = 3x^2 + 2x + 1 (example function)
    return 3 * x**2 + 2 * x + 1


# =========================
# Data Processing Function
# =========================
def data_process(n=1000, random_seed=42):
    """
    n           : total number of samples
    random_seed : reproducibility control
    """

    # ---------- Random seed (reproducibility)
    np.random.seed(random_seed)

    # ---------- Generate random x values
    x = np.random.randint(0, n, n).astype(np.float32)

    # ---------- Generate y using polynomial function
    y = my_polynomial(x).astype(np.float32)

    # =========================
    # DATA SCALING
    # =========================

    # Maximum values
    max_x = x.max()
    max_y = y.max()

    # Scale to 0â€“1 range
    x_scaled = x / max_x
    y_scaled = y / max_y

    # Reshape for ML models (samples, features)
    x_scaled = x_scaled.reshape(-1, 1)
    y_scaled = y_scaled.reshape(-1, 1)

    # =========================
    # DATA SPLITTING
    # =========================

    # ---- Step 1: Split into (train+val) and test
    # 80% train+val, 20% test
    x_train_val, x_test, y_train_val, y_test = train_test_split(
        x_scaled,
        y_scaled,
        test_size=0.2,          # 20% test data
        random_state=random_seed
    )

    # ---- Step 2: Split train+val into train and validation
    # 80% of remaining â†’ 70% train, 10% val
    x_train, x_val, y_train, y_val = train_test_split(
        x_train_val,
        y_train_val,
        test_size=0.125,        # 10% of total data
        random_state=random_seed
    )

    # =========================
    # Print dataset sizes
    # =========================
    print(f"Train samples      : {len(x_train)}")
    print(f"Validation samples : {len(x_val)}")
    print(f"Test samples       : {len(x_test)}")

    return x_train, y_train, x_val, y_val, x_test, y_test


# =========================
# Run the function
# =========================
x_train, y_train, x_val, y_val, x_test, y_test = data_process()
```

---

# ğŸ§  STEP-BY-STEP EXPLANATION

---

## ğŸ”¹ 1. Random Seed

```python
np.random.seed(random_seed)
```

ğŸ‘‰ à¦à¦•à¦‡ seed à¦¦à¦¿à¦²à§‡ **same random data à¦¬à¦¾à¦°à¦¬à¦¾à¦° à¦ªà¦¾à¦“à§Ÿà¦¾ à¦¯à¦¾à§Ÿ**

âœ” debugging
âœ” experiment reproducibility

---

## ğŸ”¹ 2. Data Generation

```python
x = np.random.randint(0, n, n)
y = my_polynomial(x)
```

* `x` â†’ random input values
* `y` â†’ known mathematical relation

ğŸ“Œ supervised learning-à¦à¦° perfect example

---

## ğŸ”¹ 3. Data Scaling (WHY?)

```python
x_scaled = x / max_x
y_scaled = y / max_y
```

### à¦•à§‡à¦¨ à¦¦à¦°à¦•à¦¾à¦°?

* Neural network à¦¬à¦¡à¦¼ à¦¸à¦‚à¦–à§à¦¯à¦¾ à¦­à¦¾à¦²à§‹ handle à¦•à¦°à§‡ à¦¨à¦¾
* Gradient stable à¦¥à¦¾à¦•à§‡
* Faster convergence

ğŸ“Œ à¦à¦Ÿà¦¾à¦•à§‡ à¦¬à¦²à§‡ **Max Scaling (0â€“1 normalization)**

---

## ğŸ”¹ 4. Reshape (VERY IMPORTANT)

```python
x_scaled.reshape(-1, 1)
```

ğŸ‘‰ ML model à¦šà¦¾à§Ÿ:

```
(samples, features)
```

âŒ reshape à¦¨à¦¾ à¦•à¦°à¦²à§‡ shape error à¦†à¦¸à¦¬à§‡

---

## ğŸ”¹ 5. `train_test_split()` â€” First Split

```python
test_size=0.2
```

ğŸ‘‰ 20% data â†’ **Test set**

| Parameter            | à¦•à¦¾à¦œ            |
| -------------------- | -------------- |
| `x_scaled, y_scaled` | input + target |
| `test_size=0.2`      | 20% test       |
| `random_state`       | same split     |

---

## ğŸ”¹ 6. Second Split (Train vs Validation)

```python
test_size=0.125
```

ğŸ‘‰ à¦•à¦¾à¦°à¦£:

```
0.125 Ã— 80% â‰ˆ 10%
```

âœ” Final ratio:

* Train = 70%
* Validation = 10%
* Test = 20%

---

## ğŸ”¹ 7. Why Validation Data?

* Model tune à¦•à¦°à¦¾à¦° à¦œà¦¨à§à¦¯
* Overfitting à¦§à¦°à¦¾à¦° à¦œà¦¨à§à¦¯
* Test data untouched à¦°à¦¾à¦–à¦¤à§‡

---

# ğŸ“Š FINAL DATA DISTRIBUTION

| Dataset    | Percentage |
| ---------- | ---------- |
| Train      | 70%        |
| Validation | 10%        |
| Test       | 20%        |

---

# âš ï¸ COMMON MISTAKES (Interview Point)

âŒ Scaling test data using test statistics
âŒ Validation data à¦¦à¦¿à§Ÿà§‡ training à¦•à¦°à¦¾
âŒ Test data repeatedly check à¦•à¦°à¦¾
âŒ `random_state` à¦¨à¦¾ à¦¦à§‡à¦“à§Ÿà¦¾

---

# ğŸ§  ONE-LINE INTERVIEW ANSWER

> We scale data to stabilize learning and split it into train, validation, and test sets to train, tune, and fairly evaluate a machine learning model.

---

