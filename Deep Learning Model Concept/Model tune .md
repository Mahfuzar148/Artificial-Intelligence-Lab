
---

# ğŸ§¾ Model Tuning (Hyperparameter Tuning) â€” Full Documentation

---

## ğŸ”¹ â€œTune à¦•à¦°à¦¾â€ à¦®à¦¾à¦¨à§‡ à¦•à§€?

ğŸ‘‰ **Model tuning** à¦®à¦¾à¦¨à§‡ à¦¹à¦²à§‹:

> model-à¦à¦° à¦à¦®à¦¨ à¦¸à¦¬ setting (hyperparameters) à¦ªà¦°à¦¿à¦¬à¦°à§à¦¤à¦¨ à¦•à¦°à¦¾
> à¦¯à§‡à¦—à§à¦²à§‹ model à¦¨à¦¿à¦œà§‡ à¦¶à§‡à¦–à§‡ à¦¨à¦¾,
> à¦•à¦¿à¦¨à§à¦¤à§ à¦¶à§‡à¦–à¦¾à¦° quality-à¦•à§‡ à¦¸à¦°à¦¾à¦¸à¦°à¦¿ à¦ªà§à¦°à¦­à¦¾à¦¬ à¦«à§‡à¦²à§‡à¥¤

ğŸ“Œ à¦¸à¦¹à¦œ à¦­à¦¾à¦·à¦¾à§Ÿ:
**â€œModel à¦•à§€à¦­à¦¾à¦¬à§‡ à¦¶à¦¿à¦–à¦¬à§‡, à¦¸à§‡à¦Ÿà¦¾ à¦ à¦¿à¦• à¦•à¦°à¦¾ = tuningâ€**

---

## ğŸ”¹ Train à¦¬à¦¨à¦¾à¦® Tune (Difference Clear)

| à¦¬à¦¿à¦·à§Ÿ       | Train         | Tune               |
| ---------- | ------------- | ------------------ |
| à¦•à§€ à¦¶à§‡à¦–à¦¾ à¦¹à§Ÿ | Weights, bias | Hyperparameters    |
| à¦•à§‡ à¦¶à§‡à¦–à§‡    | Model à¦¨à¦¿à¦œà§‡    | à¦†à¦®à¦°à¦¾ (developer)   |
| à¦•à§‹à¦¨ data   | Train data    | Validation data    |
| Goal       | Loss à¦•à¦®à¦¾à¦¨à§‹    | Best configuration |

---

## ğŸ”¹ Hyperparameter à¦•à§€?

ğŸ‘‰ Hyperparameter à¦¹à¦²à§‹ à¦à¦®à¦¨ parameterâ€”

* training à¦¶à§à¦°à§ à¦¹à¦“à§Ÿà¦¾à¦° **à¦†à¦—à§‡ à¦¸à§‡à¦Ÿ à¦•à¦°à¦¾ à¦¹à§Ÿ**
* training à¦šà¦²à¦¾à¦•à¦¾à¦²à§€à¦¨ **update à¦¹à§Ÿ à¦¨à¦¾**

---

## ğŸ”¹ Common Hyperparameters (Most Important List)

1ï¸âƒ£ Learning Rate
2ï¸âƒ£ Number of Epochs
3ï¸âƒ£ Batch Size
4ï¸âƒ£ Number of Layers
5ï¸âƒ£ Number of Neurons
6ï¸âƒ£ Activation Function
7ï¸âƒ£ Optimizer Type
8ï¸âƒ£ Regularization (L2, Dropout)
9ï¸âƒ£ EarlyStopping settings

---

# ğŸ”· 1ï¸âƒ£ Learning Rate (à¦¸à¦¬à¦šà§‡à§Ÿà§‡ à¦—à§à¦°à§à¦¤à§à¦¬à¦ªà§‚à¦°à§à¦£)

### ğŸ”¹ à¦•à§€?

ğŸ‘‰ Weight update à¦•à¦¤ à¦¬à§œ step-à¦ à¦¹à¦¬à§‡

```python
optimizer = Adam(learning_rate=0.001)
```

### ğŸ”¹ Tune à¦¨à¦¾ à¦•à¦°à¦²à§‡ à¦•à§€ à¦¸à¦®à¦¸à§à¦¯à¦¾?

| LR      | Problem           |
| ------- | ----------------- |
| à¦–à§à¦¬ à¦¬à§œ  | Loss diverge      |
| à¦–à§à¦¬ à¦›à§‹à¦Ÿ | Training à¦–à§à¦¬ slow |
| à¦ à¦¿à¦•     | Smooth learning   |

### ğŸ”¹ Tuning example

```python
LR = 0.01  â†’ val_loss = 0.45 âŒ
LR = 0.001 â†’ val_loss = 0.18 âœ…
```

---

# ğŸ”· 2ï¸âƒ£ Epochs

### ğŸ”¹ à¦•à§€?

ğŸ‘‰ à¦ªà§à¦°à§‹ dataset à¦•à¦¤à¦¬à¦¾à¦° model à¦¦à§‡à¦–à¦¬à§‡

```python
epochs = 200
```

### ğŸ”¹ Tune à¦¨à¦¾ à¦•à¦°à¦²à§‡?

* à¦•à¦® epoch â†’ underfitting
* à¦¬à§‡à¦¶à¦¿ epoch â†’ overfitting

### ğŸ”¹ Tuning example

```python
epochs = 50  â†’ val_loss = 0.30
epochs = 120 â†’ val_loss = 0.20 âœ…
```

---

# ğŸ”· 3ï¸âƒ£ Batch Size

### ğŸ”¹ à¦•à§€?

ğŸ‘‰ à¦à¦•à¦¬à¦¾à¦°à§‡ à¦•à¦¤ sample à¦¦à¦¿à§Ÿà§‡ weight update à¦¹à¦¬à§‡

```python
batch_size = 32
```

### ğŸ”¹ Effect

| Batch Size | Result       |
| ---------- | ------------ |
| à¦›à§‹à¦Ÿ        | Stable, slow |
| à¦¬à§œ         | Fast, noisy  |

### ğŸ”¹ Tuning example

```python
batch=16 â†’ val_acc=87%
batch=32 â†’ val_acc=90% âœ…
```

---

# ğŸ”· 4ï¸âƒ£ Number of Layers

### ğŸ”¹ à¦•à§€?

ğŸ‘‰ Model à¦•à¦¤à¦Ÿà¦¾ deep à¦¹à¦¬à§‡

```python
Dense â†’ Dense â†’ Dense
```

### ğŸ”¹ Tune à¦¨à¦¾ à¦•à¦°à¦²à§‡?

* à¦•à¦® layer â†’ underfitting
* à¦¬à§‡à¦¶à¦¿ layer â†’ overfitting

---

# ğŸ”· 5ï¸âƒ£ Number of Neurons

### ğŸ”¹ à¦•à§€?

ğŸ‘‰ à¦ªà§à¦°à¦¤à¦¿à¦Ÿà¦¾ layer à¦•à¦¤à¦Ÿà¦¾ capacity à¦°à¦¾à¦–à¦¬à§‡

```python
Dense(8)
Dense(32)
```

### ğŸ”¹ Tuning example

```python
Dense(4)  â†’ val_loss = 0.40 âŒ
Dense(16) â†’ val_loss = 0.19 âœ…
```

---

# ğŸ”· 6ï¸âƒ£ Activation Function

### ğŸ”¹ à¦•à§€?

ğŸ‘‰ Non-linearity à¦¯à§‹à¦— à¦•à¦°à§‡

| Activation | Use               |
| ---------- | ----------------- |
| ReLU       | Hidden layer      |
| Sigmoid    | Binary output     |
| Softmax    | Multi-class       |
| Linear     | Regression output |

### ğŸ”¹ Wrong activation à¦¦à¦¿à¦²à§‡?

âŒ Model learn à¦•à¦°à¦¤à§‡ à¦ªà¦¾à¦°à¦¬à§‡ à¦¨à¦¾

---

# ğŸ”· 7ï¸âƒ£ Optimizer

### ğŸ”¹ à¦•à§€?

ğŸ‘‰ Weight update à¦•à¦°à¦¾à¦° algorithm

| Optimizer | Use                 |
| --------- | ------------------- |
| Adam      | Default / fast      |
| SGD       | Controlled learning |
| RMSprop   | Sequence data       |

### ğŸ”¹ Tuning example

```python
SGD  â†’ slow convergence
Adam â†’ fast convergence âœ…
```

---

# ğŸ”· 8ï¸âƒ£ Regularization (Overfitting Control)

## ğŸ”¸ Dropout

```python
Dropout(0.3)
```

ğŸ‘‰ 30% neuron randomly à¦¬à¦¨à§à¦§

## ğŸ”¸ L2 Regularization

```python
Dense(16, kernel_regularizer=l2(0.001))
```

### ğŸ”¹ Tune à¦¨à¦¾ à¦•à¦°à¦²à§‡?

* Overfitting à¦¹à¦¬à§‡

---

# ğŸ”· 9ï¸âƒ£ EarlyStopping (Auto Tuning)

```python
EarlyStopping(
    monitor='val_loss',
    patience=10,
    restore_best_weights=True
)
```

ğŸ‘‰ Model à¦¨à¦¿à¦œà§‡à¦‡ à¦¬à¦²à§‡ à¦¦à§‡à§Ÿ:

> â€œà¦†à¦° improve à¦¹à¦šà§à¦›à§‡ à¦¨à¦¾â€

---

# ğŸ”¹ Tune à¦•à¦°à¦¾à¦° à¦¸à¦®à§Ÿ à¦•à§‹à¦¨ data à¦¬à§à¦¯à¦¬à¦¹à¦¾à¦° à¦¹à¦¬à§‡?

| Data       | Purpose          |
| ---------- | ---------------- |
| Train      | Weights à¦¶à§‡à¦–à¦¾     |
| Validation | ğŸ”§ Tuning        |
| Test       | Final evaluation |

âŒ Test data à¦¦à¦¿à§Ÿà§‡ tuning à¦•à¦°à¦¾ **strictly forbidden**

---

# ğŸ”„ Manual Tuning Process (Step-by-Step)

```text
1. Train model
2. Check validation metric
3. Change ONE hyperparameter
4. Train again
5. Compare results
6. Keep best configuration
```

---

# ğŸ§ª Mini End-to-End Tuning Example

```python
# Try LR = 0.01
model.compile(optimizer=Adam(0.01), loss='mse')
h1 = model.fit(...)

# Try LR = 0.001
model.compile(optimizer=Adam(0.001), loss='mse')
h2 = model.fit(...)
```

```python
min(h1.history['val_loss']),
min(h2.history['val_loss'])
```

---

# âš ï¸ Common Mistakes (VERY IMPORTANT)

âŒ Test data à¦¦à¦¿à§Ÿà§‡ tune à¦•à¦°à¦¾
âŒ à¦à¦•à¦¸à¦¾à¦¥à§‡ à¦…à¦¨à§‡à¦• hyperparameter change à¦•à¦°à¦¾
âŒ EarlyStopping à¦¬à§à¦¯à¦¬à¦¹à¦¾à¦° à¦¨à¦¾ à¦•à¦°à¦¾
âŒ Best model save à¦¨à¦¾ à¦•à¦°à¦¾

---

# ğŸ§  Best Practices (Industry)

âœ” One parameter at a time
âœ” Validation-based decision
âœ” EarlyStopping + ModelCheckpoint
âœ” Log everything
âœ” Fix random seed

---

# ğŸ§  Interview One-liners

* Tuning adjusts hyperparameters, not weights
* Validation data is used for tuning
* Learning rate is the most critical hyperparameter
* EarlyStopping is an automatic tuning method

---

# ğŸ“Œ Final Summary Table

| Aspect         | Meaning            |
| -------------- | ------------------ |
| Train          | Learn weights      |
| Tune           | Adjust settings    |
| Hyperparameter | Predefined control |
| Validation     | Tuning data        |
| Test           | Final check        |

---

## ğŸ Golden Rule

> **Train on train set, tune on validation set, evaluate on test set.**

---

