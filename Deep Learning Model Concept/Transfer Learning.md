

* âœ… Pre-trained VGG16 (ImageNet)
* âœ… include_top=False
* âœ… Base model freeze
* âœ… Custom classifier add
* âœ… Compile
* âœ… Train
* âœ… Evaluate

---

# ğŸ“˜ Transfer Learning Full Code Example (VGG16)

---

## ğŸŸ¢ Step 1: Import Libraries

```python
import tensorflow as tf
import numpy as np
from tensorflow.keras.applications import VGG16
from tensorflow.keras.layers import Dense, Flatten, Input
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
```

---

## ğŸŸ¢ Step 2: Load Example Data (Dummy Example)

à¦à¦–à¦¾à¦¨à§‡ demonstration à¦à¦° à¦œà¦¨à§à¦¯ random data à¦¬à§à¦¯à¦¬à¦¹à¦¾à¦° à¦•à¦°à¦›à¦¿à¥¤

```python
# Dummy dataset (100 images)
x_train = np.random.rand(100, 224, 224, 3)
y_train = tf.keras.utils.to_categorical(np.random.randint(0, 5, 100), 5)

x_test = np.random.rand(20, 224, 224, 3)
y_test = tf.keras.utils.to_categorical(np.random.randint(0, 5, 20), 5)
```

---

## ğŸŸ¢ Step 3: Load Pre-trained VGG16

```python
base_model = VGG16(
    weights='imagenet',
    include_top=False,
    input_shape=(224, 224, 3)
)
```

---

### ğŸ” Explanation:

* weights='imagenet' â†’ Pre-trained weights load
* include_top=False â†’ Fully connected layer à¦¬à¦¾à¦¦
* à¦à¦–à¦¨ à¦¶à§à¦§à§à¦®à¦¾à¦¤à§à¦° convolutional feature extractor à¦¥à¦¾à¦•à¦¬à§‡

---

## ğŸŸ¢ Step 4: Freeze Base Model

```python
base_model.trainable = False
```

### à¦•à§‡à¦¨?

Pre-trained weights à¦¯à§‡à¦¨ à¦ªà¦°à¦¿à¦¬à¦°à§à¦¤à¦¨ à¦¨à¦¾ à¦¹à§Ÿà¥¤

---

## ğŸŸ¢ Step 5: Add Custom Classifier

```python
inputs = base_model.input
x = base_model.output

x = Flatten()(x)
x = Dense(256, activation='relu')(x)
outputs = Dense(5, activation='softmax')(x)

model = Model(inputs, outputs)
```

---

### ğŸ” Structure à¦à¦–à¦¨ à¦à¦®à¦¨:

```
Input â†’ VGG16 Conv Blocks â†’ Flatten â†’ Dense â†’ Output
```

---

## ğŸŸ¢ Step 6: Compile Model

```python
model.compile(
    optimizer=Adam(learning_rate=0.001),
    loss='categorical_crossentropy',
    metrics=['accuracy']
)
```

---

## ğŸŸ¢ Step 7: Train Model

```python
history = model.fit(
    x_train,
    y_train,
    epochs=5,
    batch_size=16,
    validation_split=0.2
)
```

---

## ğŸŸ¢ Step 8: Evaluate Model

```python
loss, acc = model.evaluate(x_test, y_test)
print("Test Accuracy:", acc)
```

---

# ğŸ”¥ Complete Final Code (All Together)

```python
import tensorflow as tf
import numpy as np
from tensorflow.keras.applications import VGG16
from tensorflow.keras.layers import Dense, Flatten
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam

# Dummy Data
x_train = np.random.rand(100, 224, 224, 3)
y_train = tf.keras.utils.to_categorical(np.random.randint(0, 5, 100), 5)

x_test = np.random.rand(20, 224, 224, 3)
y_test = tf.keras.utils.to_categorical(np.random.randint(0, 5, 20), 5)

# Load Pre-trained Model
base_model = VGG16(
    weights='imagenet',
    include_top=False,
    input_shape=(224, 224, 3)
)

# Freeze Base
base_model.trainable = False

# Add Custom Layers
x = base_model.output
x = Flatten()(x)
x = Dense(256, activation='relu')(x)
outputs = Dense(5, activation='softmax')(x)

model = Model(base_model.input, outputs)

# Compile
model.compile(
    optimizer=Adam(learning_rate=0.001),
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

# Train
model.fit(
    x_train,
    y_train,
    epochs=5,
    batch_size=16,
    validation_split=0.2
)

# Evaluate
loss, acc = model.evaluate(x_test, y_test)
print("Test Accuracy:", acc)
```

---

# ğŸ§  Transfer Learning Concept Summary

### ğŸ”¹ Phase 1 â†’ Feature Extraction

* Base model freeze
* Only new Dense layers train

### ğŸ”¹ Phase 2 â†’ Fine-tuning (Optional)

* à¦•à¦¿à¦›à§ convolution layer unfreeze à¦•à¦°à¦¾
* Small learning rate à¦¦à¦¿à§Ÿà§‡ train

---

# ğŸ”µ Fine-Tuning Example (Advanced)

```python
base_model.trainable = True

for layer in base_model.layers[:-4]:
    layer.trainable = False

model.compile(
    optimizer=Adam(learning_rate=1e-5),
    loss='categorical_crossentropy',
    metrics=['accuracy']
)
```

---

# ğŸ¯ When To Use Transfer Learning?

* Dataset à¦›à§‹à¦Ÿ à¦¹à¦²à§‡
* Faster training à¦šà¦¾à¦‡à¦²à§‡
* High accuracy à¦¦à¦°à¦•à¦¾à¦° à¦¹à¦²à§‡

---

# ğŸ“ Viva Ready Answer

> Transfer learning à¦¹à¦²à§‹ à¦à¦®à¦¨ à¦à¦•à¦Ÿà¦¿ technique à¦¯à§‡à¦–à¦¾à¦¨à§‡ pre-trained model-à¦à¦° convolutional à¦…à¦‚à¦¶ à¦¬à§à¦¯à¦¬à¦¹à¦¾à¦° à¦•à¦°à§‡ à¦¨à¦¤à§à¦¨ dataset-à¦à¦° à¦œà¦¨à§à¦¯ custom classifier à¦¯à§‹à¦— à¦•à¦°à¦¾ à¦¹à§Ÿà¥¤ à¦à¦¤à§‡ training time à¦•à¦® à¦²à¦¾à¦—à§‡ à¦à¦¬à¦‚ accuracy à¦­à¦¾à¦²à§‹ à¦¹à§Ÿà¥¤

---

---

# ğŸ“˜ COMPLETE DOCUMENTATION: TRANSFER LEARNING

---

# ğŸ§  1ï¸âƒ£ What is Transfer Learning?

Transfer Learning is a deep learning technique where:

> A model trained on a large dataset is reused for a different but related task.

Instead of training a neural network from scratch, we:

* Use a **pre-trained model**
* Reuse its learned features
* Add a new classifier head
* Train only required parts

---

# ğŸ”µ 2ï¸âƒ£ Why Transfer Learning?

Training CNN from scratch requires:

* Large dataset (millions of images)
* High GPU power
* Long training time

Transfer Learning solves:

| Problem       | Solution                  |
| ------------- | ------------------------- |
| Small dataset | Use pre-trained features  |
| Slow training | Train fewer layers        |
| Overfitting   | Freeze base model         |
| Low accuracy  | Reuse high-level features |

---

# ğŸ— 3ï¸âƒ£ Basic Structure of Transfer Learning

```
Input
â†“
Pre-trained Feature Extractor (Frozen)
â†“
New Custom Dense Head
â†“
Output
```

---

# ğŸ” 4ï¸âƒ£ Key Terminology

| Term              | Meaning                             |
| ----------------- | ----------------------------------- |
| Backbone          | Pre-trained CNN (VGG, ResNet, etc.) |
| Head              | Newly added Dense layers            |
| Feature Extractor | Convolutional layers                |
| Freeze            | Do not update weights               |
| Fine-Tuning       | Unfreeze some layers and retrain    |

---

# ğŸŸ¢ 5ï¸âƒ£ Types of Transfer Learning

---

## ğŸŸ¡ Type 1: Feature Extraction (Most Common)

* Freeze entire base model
* Train only new Dense layers

âœ” Fast
âœ” Safe
âœ” Good for small datasets

---

## ğŸ”´ Type 2: Fine-Tuning

* Freeze most layers
* Unfreeze last few layers
* Train with small learning rate

âœ” Better accuracy
âœ” Slightly slower

---

# ğŸ§  6ï¸âƒ£ Why It Works?

Pre-trained CNN learns:

Layer 1 â†’ Edge
Layer 2 â†’ Texture
Layer 3 â†’ Shape
Layer 4 â†’ Object parts

These features are general and reusable.

---

# ğŸ“Š 7ï¸âƒ£ Real Example: VGG16 Transfer Learning

---

## ğŸ”¹ Step 1: Import

```python
import tensorflow as tf
from tensorflow.keras.applications import VGG16
from tensorflow.keras.layers import Dense, Flatten
from tensorflow.keras.models import Model
```

---

## ğŸ”¹ Step 2: Load Pre-trained Model

```python
base_model = VGG16(
    weights='imagenet',
    include_top=False,
    input_shape=(224,224,3)
)
```

### Explanation:

* weights='imagenet' â†’ Pre-trained weights
* include_top=False â†’ Remove original classifier
* input_shape â†’ Required size

---

## ğŸ”¹ Step 3: Freeze Base Model

```python
base_model.trainable = False
```

Meaning:

Only new layers will learn.

---

## ğŸ”¹ Step 4: Add New Classifier Head

```python
x = base_model.output
x = Flatten()(x)
x = Dense(256, activation='relu')(x)
outputs = Dense(10, activation='softmax')(x)

model = Model(base_model.input, outputs)
```

---

## ğŸ”¹ Step 5: Compile

```python
model.compile(
    optimizer='adam',
    loss='categorical_crossentropy',
    metrics=['accuracy']
)
```

---

## ğŸ”¹ Step 6: Train (Transfer Learning Phase)

```python
model.fit(trainX, trainY, validation_split=0.1, epochs=10)
```

Only Dense layers update.

---

# ğŸ”µ 8ï¸âƒ£ Fine-Tuning Phase

After initial training:

Unfreeze some layers.

```python
for layer in base_model.layers[-4:]:
    layer.trainable = True

model.compile(
    optimizer=tf.keras.optimizers.Adam(1e-5),
    loss='categorical_crossentropy',
    metrics=['accuracy']
)
```

Train again:

```python
model.fit(trainX, trainY, validation_split=0.1, epochs=10)
```

---

# ğŸ“ˆ 9ï¸âƒ£ Why Lower Learning Rate in Fine-Tuning?

Because:

* Pre-trained weights already good
* Large learning rate may destroy them

So we use:

```
1e-5 or 1e-6
```

---

# ğŸ§® 10ï¸âƒ£ Freezing Layers Example

From your screenshot:

```python
for layer in model.layers[:-4]:
    layer.trainable = False
```

Meaning:

* Freeze feature extractor
* Train only last 4 layers

---

# ğŸ§  11ï¸âƒ£ Transfer Learning Workflow

### Phase 1:

âœ” Load pre-trained model
âœ” Freeze backbone
âœ” Train new head

### Phase 2:

âœ” Unfreeze few layers
âœ” Reduce learning rate
âœ” Retrain

---

# ğŸ“Š 12ï¸âƒ£ Parameter Understanding

Suppose:

VGG16 = 14 million params
New head = 500k params

After freezing:

Only 500k trainable.

This reduces:

* Overfitting
* Training time
* GPU memory

---

# ğŸ”¬ 13ï¸âƒ£ When To Use Transfer Learning?

Use when:

* Dataset small (<10k images)
* Task similar to ImageNet
* Faster training needed

Do NOT use when:

* Completely different data (e.g., medical grayscale CT)
* Huge dataset available

---

# ğŸ“˜ 14ï¸âƒ£ Transfer Learning vs Training From Scratch

| Feature       | Transfer Learning | From Scratch |
| ------------- | ----------------- | ------------ |
| Data Required | Low               | Very High    |
| Training Time | Fast              | Slow         |
| Accuracy      | High              | Needs tuning |
| Overfitting   | Low               | High         |

---

# ğŸ¯ 15ï¸âƒ£ Common Pre-trained Models

| Model        | Params         | Speed    |
| ------------ | -------------- | -------- |
| VGG16        | 138M           | Slow     |
| ResNet50     | 25M            | Balanced |
| MobileNet    | 4M             | Fast     |
| EfficientNet | Very Efficient | Best     |

---

# ğŸ“ Viva Ready Explanation

> Transfer learning is a technique where a model trained on a large dataset such as ImageNet is reused for a new task. The convolutional layers are used as a feature extractor and a new classifier is added. Initially, the base model is frozen, and later fine-tuning can be applied by unfreezing some layers.

---

# ğŸ”¥ 16ï¸âƒ£ Common Mistakes

âŒ Forgetting to freeze base model
âŒ Using large learning rate in fine-tuning
âŒ Not matching input size
âŒ Not removing include_top

---

# ğŸš€ 17ï¸âƒ£ Full Professional Template

```python
base_model = VGG16(
    weights='imagenet',
    include_top=False,
    input_shape=(224,224,3)
)

base_model.trainable = False

x = base_model.output
x = Flatten()(x)
x = Dense(256, activation='relu')(x)
outputs = Dense(5, activation='softmax')(x)

model = Model(base_model.input, outputs)

model.compile(
    optimizer=tf.keras.optimizers.Adam(),
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

model.fit(trainX, trainY, validation_split=0.1, epochs=10)
```

---

# ğŸ§  Final Concept Summary

Transfer Learning =

```
Pre-trained Features + New Classifier
```

Fine-Tuning =

```
Unfreeze Few Layers + Small Learning Rate
```

---


