
---

# ğŸ“˜ Activation Function â€“ Full Documentation (Deep Learning)

---

## ğŸ”¹ 1. Activation Function à¦•à§€?

**Activation function** à¦¹à¦²à§‹ à¦à¦®à¦¨ à¦à¦•à¦Ÿà¦¿ function à¦¯à¦¾:

* Neuron-à¦à¦° output à¦•à§‡ **non-linear** à¦•à¦°à§‡
* Neural Network-à¦•à§‡ **complex pattern à¦¶à§‡à¦–à¦¾à¦° à¦•à§à¦·à¦®à¦¤à¦¾ à¦¦à§‡à§Ÿ**

ğŸ“Œ Activation à¦›à¦¾à§œà¦¾ neural network à¦¶à§à¦§à§ **linear model** à¦¹à§Ÿà¥¤

---

## ğŸ”¹ 2. Neuron Without vs With Activation

### âŒ Without Activation

[
y = Wx + b
]
â†’ Linear model
â†’ Deep à¦¹à¦²à§‡à¦“ shallow à¦à¦° à¦®à¦¤à§‹ behave à¦•à¦°à§‡

### âœ… With Activation

[
y = f(Wx + b)
]
â†’ Non-linear
â†’ Real-world problem solve à¦•à¦°à§‡

---

## ğŸ”¹ 3. Activation Function à¦•à§‹à¦¥à¦¾à§Ÿ à¦¬à¦¸à§‡?

```
Input â†’ Dense â†’ Activation â†’ Output
```

à¦¬à¦¾

```python
Dense(64, activation='relu')
```

---

## ğŸ”¹ 4. à¦•à§‡à¦¨ Activation à¦¦à¦°à¦•à¦¾à¦°?

| Without Activation   | With Activation       |
| -------------------- | --------------------- |
| Only linear          | Non-linear            |
| XOR solve âŒ          | XOR solve âœ…           |
| Deep network useless | Deep network powerful |

---

# ğŸ”¥ 5. Types of Activation Functions (à¦¸à¦¬ à¦—à§à¦°à§à¦¤à§à¦¬à¦ªà§‚à¦°à§à¦£)

---

## 1ï¸âƒ£ Linear (No Activation)

### ğŸ“ Formula

[
f(x) = x
]

### âœ… Keras

```python
Dense(1, activation='linear')
```

### ğŸ”¹ Use

* Regression
* Price / temperature prediction

### âš ï¸ Note

* Hidden layer-à¦ à¦¬à§à¦¯à¦¬à¦¹à¦¾à¦° à¦•à¦°à¦¾ à¦¹à§Ÿ à¦¨à¦¾

---

## 2ï¸âƒ£ Sigmoid

### ğŸ“ Formula

[
f(x) = \frac{1}{1 + e^{-x}}
]

### ğŸ“Š Range

```
(0, 1)
```

### âœ… Keras

```python
Dense(1, activation='sigmoid')
```

### ğŸ”¹ Use

* Binary classification
* Probability output

### âŒ Problems

* Vanishing gradient
* Slow training

---

## 3ï¸âƒ£ Tanh

### ğŸ“ Formula

[
f(x) = \tanh(x)
]

### ğŸ“Š Range

```
(-1, 1)
```

### âœ… Keras

```python
Dense(64, activation='tanh')
```

### ğŸ”¹ Use

* Hidden layers (older models)
* RNN

### âŒ Problem

* Vanishing gradient

---

## 4ï¸âƒ£ ReLU â­ (Most Popular)

### ğŸ“ Formula

[
f(x) = \max(0, x)
]

### ğŸ“Š Range

```
[0, âˆ)
```

### âœ… Keras

```python
Dense(64, activation='relu')
```

### ğŸ”¹ Use

* Hidden layers
* CNN / DNN

### âŒ Problem

* Dead neuron problem

---

## 5ï¸âƒ£ Leaky ReLU

### ğŸ“ Formula

[
f(x) =
\begin{cases}
x, & x>0 \
\alpha x, & x\le 0
\end{cases}
]

### âœ… Keras

```python
from tensorflow.keras.layers import LeakyReLU
LeakyReLU(alpha=0.1)
```

### ğŸ”¹ Use

* ReLU dead neuron problem solve à¦•à¦°à¦¤à§‡

---

## 6ï¸âƒ£ PReLU

### ğŸ“ Formula

* Î± **learnable**

### âœ… Keras

```python
from tensorflow.keras.layers import PReLU
PReLU()
```

### ğŸ”¹ Use

* When model needs adaptive slope

---

## 7ï¸âƒ£ ELU

### ğŸ“ Formula

* Smooth negative output

### âœ… Keras

```python
Dense(64, activation='elu')
```

### ğŸ”¹ Use

* Faster convergence than ReLU (some cases)

---

## 8ï¸âƒ£ Softmax â­

### ğŸ“ Formula

[
f(x_i) = \frac{e^{x_i}}{\sum e^{x_j}}
]

### ğŸ“Š Output

```
Sum of probabilities = 1
```

### âœ… Keras

```python
Dense(10, activation='softmax')
```

### ğŸ”¹ Use

* Multi-class classification (output layer only)

---

## 9ï¸âƒ£ Swish

### ğŸ“ Formula

[
f(x) = x \cdot \sigma(x)
]

### âœ… Keras

```python
Dense(64, activation='swish')
```

### ğŸ”¹ Use

* Modern deep networks
* EfficientNet

---

## ğŸ”Ÿ GELU

### ğŸ“ Formula

* Gaussian based

### âœ… Keras

```python
Dense(64, activation='gelu')
```

### ğŸ”¹ Use

* Transformers
* NLP models (BERT)

---

# ğŸ§  6. Activation Selection Rule (Exam Important)

| Layer Type         | Best Activation |
| ------------------ | --------------- |
| Hidden layer       | ReLU            |
| Binary output      | Sigmoid         |
| Multi-class output | Softmax         |
| Regression output  | Linear          |
| RNN                | Tanh            |
| Transformer        | GELU            |

---

# âŒ 7. Common Mistakes

### âŒ Softmax in hidden layer

```python
Dense(64, activation='softmax')  # WRONG
```

---

### âŒ Sigmoid for multi-class

```python
Dense(3, activation='sigmoid')  # WRONG
```

---

### âŒ No activation at all

```python
Dense(64)  # Weak model
```

---

# ğŸ§ª 8. Activation Example Model

```python
inputs = Input((10,))
x = Dense(64, activation='relu')(inputs)
x = Dense(32, activation='relu')(x)
outputs = Dense(1, activation='sigmoid')(x)
model = Model(inputs, outputs)
```

---

# ğŸ“Œ 9. Activation Cheat Sheet

| Task        | Activation |
| ----------- | ---------- |
| Regression  | Linear     |
| Binary      | Sigmoid    |
| Multi-class | Softmax    |
| Hidden      | ReLU       |
| NLP         | GELU       |

---

## âœ… Final Summary

* Activation adds **non-linearity**
* Without activation â†’ deep model useless
* ReLU is default for hidden layers
* Output activation depends on problem type

---

