---

# üìö Fully Connected Neural Network (FCNN) 

---

## **1. Introduction**

A **Fully Connected Neural Network** (also called a **Dense Neural Network**) is a type of Artificial Neural Network where **each neuron in one layer is connected to every neuron in the next layer**.

* Each connection has:

  * **Weight (w)**: Determines the strength and direction of the connection.
  * **Bias (b)**: Allows shifting of the activation function.
* An **activation function** is applied to the neuron‚Äôs output to introduce non-linearity.

üí° **Key idea:** This structure ensures that every feature from one layer is considered in the next layer‚Äôs calculations.

---

## **2. Architecture**

A typical FCNN has:

1. **Input Layer**

   * Receives the data features.
   * Each node represents one input feature.

2. **Hidden Layers**

   * One or more fully connected layers.
   * Each neuron receives inputs from all neurons in the previous layer.
   * Each layer applies a **weighted sum + bias**, then passes it through an **activation function**.

3. **Output Layer**

   * Produces the final prediction (classification or regression).
   * Uses an appropriate activation function:

     * **Softmax** ‚Üí Multi-class classification
     * **Sigmoid** ‚Üí Binary classification
     * **Linear** ‚Üí Regression

---

## **3. How It Works ‚Äî Step-by-Step**

1. **Forward Pass**

   * For each neuron in a layer:

     $$
     z = \sum_{i=1}^{n} w_i \cdot x_i + b
     $$
   * Apply the activation function:

     $$
     a = f(z)
     $$

     where $f$ can be ReLU, Sigmoid, Softmax, etc.

2. **Loss Calculation**

   * Compare the predicted output with the actual target using a loss function (e.g., MSE, Cross-Entropy).

3. **Backpropagation**

   * Compute gradients of the loss with respect to each weight and bias.
   * Use the **chain rule** to propagate the error backwards through the network.

4. **Weight Update**

   * Update weights and biases using an optimization algorithm such as Gradient Descent:

     $$
     w \leftarrow w - \eta \cdot \frac{\partial L}{\partial w}
     $$

     where $\eta$ is the learning rate.

---

## **4. Activation Functions in FCNN**

* **ReLU**: $f(z) = \max(0, z)$ ‚Üí prevents vanishing gradients, widely used in hidden layers.
* **Sigmoid**: Outputs values between 0 and 1 ‚Üí useful for binary outputs.
* **Softmax**: Converts outputs into probabilities that sum to 1 ‚Üí used for multi-class classification.

---

## **5. Advantages**

‚úÖ Simple and easy to implement.
‚úÖ Can model complex relationships if enough layers and neurons are used.
‚úÖ Works well for structured/tabular data.
‚úÖ Serves as a foundation for more complex networks (CNNs, RNNs).

---

## **6. Disadvantages**

‚ö†Ô∏è Large number of parameters ‚Üí can lead to overfitting.
‚ö†Ô∏è Inefficient for image or sequential data (better to use CNNs or RNNs).
‚ö†Ô∏è Computationally expensive for very large input sizes.

---

## **7. When to Use**

* Tabular datasets (structured data).
* Problems where all features are important.
* Small to medium-sized datasets.
* As the classification/regression head in more complex architectures.

---

## **8. Python Example with Keras**

```python
from tensorflow.keras.layers import Dense, Input
from tensorflow.keras.models import Model

# Input layer with 2 features
inputs = Input((2,))

# Hidden layers
h1 = Dense(2, activation='relu')(inputs)
h2 = Dense(3, activation='relu')(h1)
h3 = Dense(2, activation='relu')(h2)

# Output layer (sigmoid for binary classification)
outputs = Dense(1, activation='sigmoid')(h3)

# Build and summarize the model
model = Model(inputs, outputs)
model.summary()
```

---

## **9. Parameter Calculation Example**

| Layer     | Inputs | Neurons | Parameters Formula | Parameters |
| --------- | ------ | ------- | ------------------ | ---------- |
| H1        | 2      | 2       | `(2 √ó 2) + 2`      | 6          |
| H2        | 2      | 3       | `(2 √ó 3) + 3`      | 9          |
| H3        | 3      | 2       | `(3 √ó 2) + 2`      | 8          |
| Out       | 2      | 1       | `(2 √ó 1) + 1`      | 3          |
| **Total** | ‚Äî      | ‚Äî       | ‚Äî                  | **26**     |

---

## **10. Visual Representation**

**Forward Pass Diagram:**

1. Input layer ‚Üí Hidden layers (fully connected) ‚Üí Output layer.
2. Every neuron in one layer connects to **every neuron** in the next layer.

---


