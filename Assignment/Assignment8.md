## ðŸ“ Question

**Build a Convolutional Neural Network (CNN) based classifier having architecture similar to the classical VGG16.**

---

# ðŸ“˜ CNN Classifier Similar to Classical VGG16

### ðŸ”¹ What does â€œsimilar to VGG16â€ mean?

Classical **VGG16** architecture characteristics:

* Small filters (3Ã—3)
* Multiple Conv layers stacked together
* After each block â†’ MaxPooling
* Fully connected layers at the end
* Softmax output for classification

We will build a **VGG-like CNN (VGG-style architecture)** for 10-class classification.

---

# ðŸ”µ Full Code (VGG-like CNN for CIFAR-10)

This example uses CIFAR-10 dataset.

---

```python
# ==========================================
# VGG-like CNN Classifier (10 Classes)
# ==========================================

import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.datasets import cifar10
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Input
from tensorflow.keras.models import Model
from tensorflow.keras.callbacks import EarlyStopping

# ======================
# Load CIFAR-10 Dataset
# ======================
(x_train, y_train), (x_test, y_test) = cifar10.load_data()

y_train = y_train.flatten()
y_test = y_test.flatten()

# Normalize
x_train = x_train / 255.0
x_test = x_test / 255.0

# ======================
# Build VGG-like Model
# ======================

inputs = Input((32,32,3))

# -------- Block 1 --------
x = Conv2D(64, (3,3), activation='relu', padding='same')(inputs)
x = Conv2D(64, (3,3), activation='relu', padding='same')(x)
x = MaxPooling2D((2,2))(x)

# -------- Block 2 --------
x = Conv2D(128, (3,3), activation='relu', padding='same')(x)
x = Conv2D(128, (3,3), activation='relu', padding='same')(x)
x = MaxPooling2D((2,2))(x)

# -------- Block 3 --------
x = Conv2D(256, (3,3), activation='relu', padding='same')(x)
x = Conv2D(256, (3,3), activation='relu', padding='same')(x)
x = Conv2D(256, (3,3), activation='relu', padding='same')(x)
x = MaxPooling2D((2,2))(x)

# -------- Block 4 --------
x = Conv2D(512, (3,3), activation='relu', padding='same')(x)
x = Conv2D(512, (3,3), activation='relu', padding='same')(x)
x = Conv2D(512, (3,3), activation='relu', padding='same')(x)
x = MaxPooling2D((2,2))(x)

# -------- Classification Head --------
x = Flatten()(x)
x = Dense(512, activation='relu')(x)
x = Dense(256, activation='relu')(x)

outputs = Dense(10, activation='softmax')(x)

model = Model(inputs, outputs, name="VGG_like_CNN")

# ======================
# Compile Model
# ======================

model.compile(
    optimizer='adam',
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

model.summary()

# ======================
# EarlyStopping
# ======================

early_stop = EarlyStopping(
    monitor='val_loss',
    patience=5,
    restore_best_weights=True
)

# ======================
# Train
# ======================

history = model.fit(
    x_train, y_train,
    epochs=50,
    batch_size=64,
    validation_split=0.1,
    callbacks=[early_stop]
)

# ======================
# Evaluate
# ======================

loss, acc = model.evaluate(x_test, y_test)
print("Test Accuracy:", acc)

# ======================
# Plot Accuracy Curve
# ======================

plt.plot(history.history['accuracy'], label='Train Acc')
plt.plot(history.history['val_accuracy'], label='Val Acc')
plt.title("Accuracy Curve")
plt.xlabel("Epoch")
plt.ylabel("Accuracy")
plt.legend()
plt.show()

# ======================
# Plot Loss Curve
# ======================

plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Val Loss')
plt.title("Loss Curve")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.legend()
plt.show()
```

---

# ðŸ“˜ Architecture Summary

VGG-style pattern:

```
[Conv â†’ Conv] â†’ MaxPool
[Conv â†’ Conv] â†’ MaxPool
[Conv â†’ Conv â†’ Conv] â†’ MaxPool
[Conv â†’ Conv â†’ Conv] â†’ MaxPool
Flatten
Dense
Dense
Output
```

---

# ðŸ”¬ Why This is VGG-like?

âœ” Uses 3Ã—3 filters
âœ” Stacked convolution layers
âœ” Block structure
âœ” MaxPooling after each block
âœ” Fully connected head

---

# ðŸ“Š Expected Performance

CIFAR-10 Accuracy â‰ˆ 75â€“85% (depending on training time & hardware)

---

# ðŸ§  Viva Ready Explanation

> A VGG-like CNN classifier was implemented using stacked 3Ã—3 convolutional layers grouped into blocks followed by max pooling layers. The architecture mimics classical VGG16 design principles and uses a fully connected classification head for 10-class prediction.

---


